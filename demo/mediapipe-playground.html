<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MediaPipe & Vision Models Playground</title>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: #1a1a2e;
      color: #eee;
      min-height: 100vh;
    }

    /* Header */
    .header {
      background: #16213e;
      padding: 16px 24px;
      display: flex;
      align-items: center;
      justify-content: space-between;
      border-bottom: 1px solid #0f3460;
    }

    .header h1 {
      font-size: 20px;
      font-weight: 500;
      color: #e94560;
    }

    .header-status {
      display: flex;
      align-items: center;
      gap: 16px;
      font-size: 13px;
    }

    .status-item {
      display: flex;
      align-items: center;
      gap: 6px;
    }

    .status-dot {
      width: 8px;
      height: 8px;
      border-radius: 50%;
      background: #666;
    }

    .status-dot.active {
      background: #4ade80;
    }

    .status-dot.loading {
      background: #fbbf24;
      animation: pulse 1s infinite;
    }

    .status-dot.error {
      background: #ef4444;
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.5; }
    }

    /* Main Layout */
    .main-container {
      display: flex;
      height: calc(100vh - 60px);
    }

    /* Sidebar */
    .sidebar {
      width: 280px;
      background: #16213e;
      border-right: 1px solid #0f3460;
      display: flex;
      flex-direction: column;
      overflow-y: auto;
    }

    .sidebar-section {
      padding: 16px;
      border-bottom: 1px solid #0f3460;
    }

    .sidebar-section h3 {
      font-size: 11px;
      text-transform: uppercase;
      letter-spacing: 1px;
      color: #888;
      margin-bottom: 12px;
    }

    /* Model Selector */
    .model-list {
      display: flex;
      flex-direction: column;
      gap: 4px;
    }

    .model-item {
      display: flex;
      align-items: center;
      padding: 10px 12px;
      border-radius: 6px;
      cursor: pointer;
      transition: background 0.2s;
      gap: 10px;
    }

    .model-item:hover {
      background: rgba(233, 69, 96, 0.1);
    }

    .model-item.active {
      background: rgba(233, 69, 96, 0.2);
      border: 1px solid #e94560;
    }

    .model-item.disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }

    .model-icon {
      width: 32px;
      height: 32px;
      border-radius: 6px;
      background: #0f3460;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 16px;
    }

    .model-info {
      flex: 1;
    }

    .model-name {
      font-size: 13px;
      font-weight: 500;
    }

    .model-status {
      font-size: 11px;
      color: #888;
    }

    .model-badge {
      font-size: 9px;
      padding: 2px 6px;
      border-radius: 3px;
      text-transform: uppercase;
      font-weight: 600;
    }

    .model-badge.current {
      background: #4ade80;
      color: #000;
    }

    .model-badge.potential {
      background: #fbbf24;
      color: #000;
    }

    /* Settings Panel */
    .settings-group {
      margin-bottom: 16px;
    }

    .settings-group label {
      display: block;
      font-size: 12px;
      color: #aaa;
      margin-bottom: 6px;
    }

    .settings-group select,
    .settings-group input[type="range"] {
      width: 100%;
      padding: 8px;
      border-radius: 4px;
      border: 1px solid #0f3460;
      background: #1a1a2e;
      color: #eee;
      font-size: 13px;
    }

    .settings-group input[type="range"] {
      padding: 0;
      height: 6px;
      -webkit-appearance: none;
      background: #0f3460;
    }

    .settings-group input[type="range"]::-webkit-slider-thumb {
      -webkit-appearance: none;
      width: 16px;
      height: 16px;
      border-radius: 50%;
      background: #e94560;
      cursor: pointer;
    }

    .range-value {
      font-size: 12px;
      color: #e94560;
      text-align: right;
      margin-top: 4px;
    }

    .toggle-row {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 8px 0;
    }

    .toggle-label {
      font-size: 13px;
    }

    .toggle-switch {
      position: relative;
      width: 40px;
      height: 22px;
    }

    .toggle-switch input {
      opacity: 0;
      width: 0;
      height: 0;
    }

    .toggle-slider {
      position: absolute;
      cursor: pointer;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: #0f3460;
      border-radius: 22px;
      transition: 0.3s;
    }

    .toggle-slider:before {
      position: absolute;
      content: "";
      height: 16px;
      width: 16px;
      left: 3px;
      bottom: 3px;
      background: #eee;
      border-radius: 50%;
      transition: 0.3s;
    }

    .toggle-switch input:checked + .toggle-slider {
      background: #e94560;
    }

    .toggle-switch input:checked + .toggle-slider:before {
      transform: translateX(18px);
    }

    /* Video Area */
    .video-area {
      flex: 1;
      display: flex;
      flex-direction: column;
      padding: 24px;
      gap: 16px;
    }

    .video-container {
      position: relative;
      flex: 1;
      background: #000;
      border-radius: 12px;
      overflow: hidden;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .video-container video,
    .video-container canvas {
      max-width: 100%;
      max-height: 100%;
      object-fit: contain;
    }

    #webcam {
      position: absolute;
      opacity: 0;
      pointer-events: none;
    }

    #output-canvas {
      position: relative;
      z-index: 1;
    }

    .video-overlay {
      position: absolute;
      top: 16px;
      left: 16px;
      display: flex;
      flex-direction: column;
      gap: 8px;
      z-index: 10;
    }

    .overlay-stat {
      background: rgba(0, 0, 0, 0.7);
      padding: 6px 12px;
      border-radius: 4px;
      font-size: 12px;
      font-family: monospace;
    }

    .overlay-stat.fps {
      color: #4ade80;
    }

    .overlay-stat.inference {
      color: #fbbf24;
    }

    .video-placeholder {
      text-align: center;
      color: #666;
    }

    .video-placeholder-icon {
      font-size: 48px;
      margin-bottom: 16px;
    }

    /* Controls Bar */
    .controls-bar {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 12px;
      padding: 16px;
      background: #16213e;
      border-radius: 8px;
    }

    .control-btn {
      padding: 12px 24px;
      border: none;
      border-radius: 6px;
      font-size: 14px;
      font-weight: 500;
      cursor: pointer;
      transition: all 0.2s;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .control-btn.primary {
      background: #e94560;
      color: white;
    }

    .control-btn.primary:hover {
      background: #d63d56;
    }

    .control-btn.secondary {
      background: #0f3460;
      color: #eee;
    }

    .control-btn.secondary:hover {
      background: #1a4a7a;
    }

    .control-btn:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }

    /* Info Panel */
    .info-panel {
      width: 320px;
      background: #16213e;
      border-left: 1px solid #0f3460;
      display: flex;
      flex-direction: column;
      overflow-y: auto;
    }

    .info-section {
      padding: 16px;
      border-bottom: 1px solid #0f3460;
    }

    .info-section h3 {
      font-size: 14px;
      font-weight: 500;
      margin-bottom: 12px;
      color: #e94560;
    }

    .info-text {
      font-size: 13px;
      line-height: 1.6;
      color: #aaa;
    }

    .info-text p {
      margin-bottom: 12px;
    }

    /* Performance Metrics */
    .metrics-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 12px;
    }

    .metric-card {
      background: #1a1a2e;
      padding: 12px;
      border-radius: 6px;
      text-align: center;
    }

    .metric-value {
      font-size: 24px;
      font-weight: 600;
      color: #e94560;
    }

    .metric-label {
      font-size: 11px;
      color: #888;
      margin-top: 4px;
    }

    /* Visualization Options */
    .viz-options {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
    }

    .viz-option {
      padding: 6px 12px;
      border-radius: 4px;
      font-size: 12px;
      background: #1a1a2e;
      border: 1px solid #0f3460;
      cursor: pointer;
      transition: all 0.2s;
    }

    .viz-option:hover {
      border-color: #e94560;
    }

    .viz-option.active {
      background: #e94560;
      border-color: #e94560;
      color: white;
    }

    /* Loading Overlay */
    .loading-overlay {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: rgba(26, 26, 46, 0.95);
      display: flex;
      align-items: center;
      justify-content: center;
      flex-direction: column;
      gap: 24px;
      z-index: 1000;
    }

    .loading-overlay.hidden {
      display: none;
    }

    .loading-spinner {
      width: 48px;
      height: 48px;
      border: 3px solid #0f3460;
      border-top-color: #e94560;
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }

    @keyframes spin {
      to { transform: rotate(360deg); }
    }

    .loading-text {
      font-size: 14px;
      color: #888;
    }

    /* Error Toast */
    .error-toast {
      position: fixed;
      bottom: 24px;
      left: 50%;
      transform: translateX(-50%);
      background: #ef4444;
      color: white;
      padding: 12px 24px;
      border-radius: 8px;
      font-size: 14px;
      z-index: 1001;
      display: none;
    }

    .error-toast.visible {
      display: block;
    }
  </style>
</head>
<body>
  <!-- Loading Overlay -->
  <div id="loading-overlay" class="loading-overlay">
    <div class="loading-spinner"></div>
    <div class="loading-text">Loading models...</div>
  </div>

  <!-- Error Toast -->
  <div id="error-toast" class="error-toast"></div>

  <!-- Header -->
  <header class="header">
    <h1>Vision Models Playground</h1>
    <div class="header-status">
      <div class="status-item">
        <span class="status-dot" id="camera-status"></span>
        <span>Camera</span>
      </div>
      <div class="status-item">
        <span class="status-dot" id="model-status"></span>
        <span id="model-status-text">No model</span>
      </div>
    </div>
  </header>

  <!-- Main Container -->
  <div class="main-container">
    <!-- Sidebar: Model Selection -->
    <aside class="sidebar">
      <div class="sidebar-section">
        <h3>Models</h3>
        <div class="model-list" id="model-list">
          <!-- Models will be populated by JS -->
        </div>
      </div>

      <div class="sidebar-section" id="model-settings">
        <h3>Settings</h3>
        <div id="settings-content">
          <p style="font-size: 12px; color: #666;">Select a model to see settings</p>
        </div>
      </div>

      <div class="sidebar-section">
        <h3>Visualization</h3>
        <div class="viz-options" id="viz-options">
          <!-- Visualization options populated by JS -->
        </div>
      </div>
    </aside>

    <!-- Video Area -->
    <main class="video-area">
      <div class="video-container" id="video-container">
        <video id="webcam" autoplay playsinline muted></video>
        <canvas id="output-canvas"></canvas>
        <div class="video-overlay">
          <div class="overlay-stat fps" id="fps-display">-- FPS</div>
          <div class="overlay-stat inference" id="inference-display">-- ms</div>
        </div>
        <div class="video-placeholder" id="video-placeholder">
          <div class="video-placeholder-icon">üì∑</div>
          <p>Click "Start Camera" to begin</p>
        </div>
      </div>

      <div class="controls-bar">
        <button class="control-btn primary" id="start-btn">
          <span>‚ñ∂</span> Start Camera
        </button>
        <button class="control-btn secondary" id="stop-btn" disabled>
          <span>‚èπ</span> Stop
        </button>
        <button class="control-btn secondary" id="screenshot-btn" disabled>
          <span>üì∏</span> Screenshot
        </button>
      </div>
    </main>

    <!-- Info Panel -->
    <aside class="info-panel">
      <div class="info-section">
        <h3>Performance</h3>
        <div class="metrics-grid">
          <div class="metric-card">
            <div class="metric-value" id="metric-fps">--</div>
            <div class="metric-label">FPS</div>
          </div>
          <div class="metric-card">
            <div class="metric-value" id="metric-inference">--</div>
            <div class="metric-label">Inference (ms)</div>
          </div>
          <div class="metric-card">
            <div class="metric-value" id="metric-resolution">--</div>
            <div class="metric-label">Resolution</div>
          </div>
          <div class="metric-card">
            <div class="metric-value" id="metric-model-size">--</div>
            <div class="metric-label">Model Size</div>
          </div>
        </div>
      </div>

      <div class="info-section">
        <h3 id="info-title">About</h3>
        <div class="info-text" id="info-content">
          <p>Select a model from the sidebar to see details about what it does and how it's used in the extension.</p>
        </div>
      </div>

      <div class="info-section">
        <h3>Usage in Extension</h3>
        <div class="info-text" id="usage-content">
          <p>Information about how the selected model is used (or could be used) in the Meet Camera Overlay extension.</p>
        </div>
      </div>
    </aside>
  </div>

  <!-- Scripts -->
  <script type="module">
    // ============================================
    // Model Registry
    // ============================================
    const MODELS = {
      'segmentation': {
        id: 'segmentation',
        name: 'Image Segmenter',
        icon: 'üë§',
        status: 'current',
        description: 'Separates people from background using MediaPipe Selfie Segmentation.',
        usage: 'Used to render wall art BEHIND the person. The segmentation mask determines which pixels belong to the person vs background.',
        modelSize: '~256 KB',
        settings: [
          { id: 'model-type', type: 'select', label: 'Model Type', options: ['Landscape', 'General'], default: 'Landscape' },
          { id: 'threshold', type: 'range', label: 'Confidence Threshold', min: 0, max: 1, step: 0.05, default: 0.5 }
        ],
        vizOptions: ['Mask Only', 'Colored Overlay', 'Background Blur', 'Background Replace']
      },
      'wall-detection': {
        id: 'wall-detection',
        name: 'Wall Detection',
        icon: 'üß±',
        status: 'current',
        description: 'Polygon-based wall detection. Finds straight lines in the background and forms rectangular wall regions.',
        usage: 'Uses person segmentation to find background, then detects straight edges to form wall polygons. Walls have straight edges that form rectangles.',
        modelSize: 'N/A (CPU)',
        settings: [
          { id: 'edge-threshold', type: 'range', label: 'Edge Sensitivity', min: 20, max: 100, step: 10, default: 40 },
          { id: 'min-line-length', type: 'range', label: 'Min Line Length (%)', min: 5, max: 30, step: 5, default: 15 },
          { id: 'angle-tolerance', type: 'range', label: 'Angle Tolerance (¬∞)', min: 2, max: 20, step: 2, default: 10 },
          { id: 'merge-distance', type: 'range', label: 'Line Merge Distance', min: 5, max: 30, step: 5, default: 15 },
          { id: 'update-interval', type: 'range', label: 'Update (frames)', min: 1, max: 10, step: 1, default: 2 }
        ],
        vizOptions: ['Wall Polygon', 'All Lines', 'H/V Lines', 'Background Mask', 'Debug']
      },
      'edge-detection': {
        id: 'edge-detection',
        name: 'Edge Detection',
        icon: 'üìê',
        status: 'current',
        description: 'Detects edges and lines in the video frame for snapping wall art boundaries.',
        usage: 'Allows wall art region corners to snap to detected edges like wall corners, shelves, and door frames.',
        modelSize: 'N/A (CPU)',
        settings: [
          { id: 'threshold', type: 'range', label: 'Detection Threshold', min: 10, max: 100, step: 5, default: 40 },
          { id: 'blur-radius', type: 'range', label: 'Blur Radius', min: 0, max: 5, step: 1, default: 1 },
          { id: 'min-length', type: 'range', label: 'Min Line Length', min: 5, max: 50, step: 5, default: 15 }
        ],
        vizOptions: ['All Edges', 'Strong Edges Only', 'Edge Angles']
      },
      'depth-estimation': {
        id: 'depth-estimation',
        name: 'Depth Estimation',
        icon: 'üåä',
        status: 'current',
        description: 'Real depth estimation using Depth Anything V2 Small model via Transformers.js. Estimates per-pixel depth.',
        usage: 'Identifies wall planes by finding regions with consistent depth. Enables accurate wall detection by distinguishing surfaces at different distances.',
        modelSize: '27 MB (INT8)',
        settings: [
          { id: 'device', type: 'select', label: 'Device', options: ['WebGPU (Fast)', 'WASM (Compatible)'], default: 'WebGPU (Fast)' },
          { id: 'update-interval', type: 'range', label: 'Update Every N Frames', min: 1, max: 60, step: 1, default: 15 },
          { id: 'resolution-scale', type: 'range', label: 'Resolution Scale', min: 0.25, max: 1, step: 0.25, default: 0.5 }
        ],
        vizOptions: ['Depth Map', 'Depth Contours', 'Near/Far Regions', 'Wall Planes']
      },
      'stabilization': {
        id: 'stabilization',
        name: 'Stabilization',
        icon: 'üéØ',
        status: 'comparison',
        description: 'Compare different approaches to keeping wall art stable when the camera or person moves.',
        usage: 'Current: Simple smoothing (jiggle compensator). Future: Feature tracking could lock art to actual wall position.',
        modelSize: 'N/A',
        settings: [
          { id: 'mode', type: 'select', label: 'Stabilization Mode', options: ['None', 'Current (Smoothing)', 'Feature Tracking (Demo)'], default: 'Current (Smoothing)' },
          { id: 'smoothing', type: 'range', label: 'Smoothing Factor', min: 0, max: 1, step: 0.1, default: 0.7 }
        ],
        vizOptions: ['Position Trail', 'Jitter Graph', 'Side-by-Side']
      },
      'combined': {
        id: 'combined',
        name: 'Combined Pipeline',
        icon: 'üîó',
        status: 'demo',
        description: 'See all models working together in the full wall art rendering pipeline.',
        usage: 'Demonstrates how segmentation, wall detection, and edge detection work together for wall art placement.',
        modelSize: 'Multiple',
        settings: [
          { id: 'show-segmentation', type: 'toggle', label: 'Show Segmentation', default: true },
          { id: 'show-walls', type: 'toggle', label: 'Show Wall Detection', default: true },
          { id: 'show-edges', type: 'toggle', label: 'Show Edges', default: false }
        ],
        vizOptions: ['Full Pipeline', 'Layers View']
      },
      'perspective-transform': {
        id: 'perspective-transform',
        name: 'Perspective Transform',
        icon: 'üñºÔ∏è',
        status: 'demo',
        description: 'Interactive demo of perspective/homography transform. Drag 4 corners to warp an image onto any quadrilateral.',
        usage: 'This is the core technique for making wall art appear correctly on angled walls. Place 4 markers on the wall, and images will be perspective-corrected to match.',
        modelSize: 'N/A (CPU)',
        settings: [
          { id: 'subdivisions', type: 'range', label: 'Mesh Subdivisions', min: 2, max: 16, step: 2, default: 8 },
          { id: 'show-grid', type: 'toggle', label: 'Show Transform Grid', default: true },
          { id: 'opacity', type: 'range', label: 'Image Opacity', min: 0.1, max: 1, step: 0.1, default: 0.8 },
          { id: 'use-segmentation', type: 'toggle', label: 'Person Occlusion', default: true },
          { id: 'pattern', type: 'select', label: 'Image Source', options: ['Starry Night', 'Great Wave', 'Mona Lisa', 'Checkerboard', 'Grid Lines', 'Gradient', 'Upload Image...'], default: 'Starry Night' }
        ],
        vizOptions: ['Transformed Image', 'Wireframe', 'UV Mapping', 'Before/After']
      }
    };

    // ============================================
    // App State
    // ============================================
    const state = {
      currentModel: null,
      isRunning: false,
      stream: null,
      animationId: null,
      fps: 0,
      inferenceTime: 0,
      frameCount: 0,
      lastFpsUpdate: 0,
      settings: {},
      vizMode: 0,
      models: {
        segmenter: null,
        depthEstimator: null
      }
    };

    // ============================================
    // DOM Elements
    // ============================================
    const elements = {
      loadingOverlay: document.getElementById('loading-overlay'),
      errorToast: document.getElementById('error-toast'),
      cameraStatus: document.getElementById('camera-status'),
      modelStatus: document.getElementById('model-status'),
      modelStatusText: document.getElementById('model-status-text'),
      modelList: document.getElementById('model-list'),
      settingsContent: document.getElementById('settings-content'),
      vizOptions: document.getElementById('viz-options'),
      videoContainer: document.getElementById('video-container'),
      webcam: document.getElementById('webcam'),
      canvas: document.getElementById('output-canvas'),
      videoPlaceholder: document.getElementById('video-placeholder'),
      fpsDisplay: document.getElementById('fps-display'),
      inferenceDisplay: document.getElementById('inference-display'),
      startBtn: document.getElementById('start-btn'),
      stopBtn: document.getElementById('stop-btn'),
      screenshotBtn: document.getElementById('screenshot-btn'),
      metricFps: document.getElementById('metric-fps'),
      metricInference: document.getElementById('metric-inference'),
      metricResolution: document.getElementById('metric-resolution'),
      metricModelSize: document.getElementById('metric-model-size'),
      infoTitle: document.getElementById('info-title'),
      infoContent: document.getElementById('info-content'),
      usageContent: document.getElementById('usage-content')
    };

    const ctx = elements.canvas.getContext('2d');

    // ============================================
    // Initialize
    // ============================================
    function init() {
      renderModelList();
      setupEventListeners();
      hideLoading();
      selectModel('segmentation');
    }

    function renderModelList() {
      elements.modelList.innerHTML = Object.values(MODELS).map(model => `
        <div class="model-item" data-model="${model.id}">
          <div class="model-icon">${model.icon}</div>
          <div class="model-info">
            <div class="model-name">${model.name}</div>
            <div class="model-status">${getStatusText(model.status)}</div>
          </div>
          <span class="model-badge ${model.status}">${model.status}</span>
        </div>
      `).join('');
    }

    function getStatusText(status) {
      switch (status) {
        case 'current': return 'In use';
        case 'potential': return 'Could add';
        case 'comparison': return 'Compare';
        case 'demo': return 'Demo';
        default: return status;
      }
    }

    function setupEventListeners() {
      // Model selection
      elements.modelList.addEventListener('click', (e) => {
        const modelItem = e.target.closest('.model-item');
        if (modelItem) {
          selectModel(modelItem.dataset.model);
        }
      });

      // Camera controls
      elements.startBtn.addEventListener('click', startCamera);
      elements.stopBtn.addEventListener('click', stopCamera);
      elements.screenshotBtn.addEventListener('click', takeScreenshot);

      // Visualization options
      elements.vizOptions.addEventListener('click', (e) => {
        if (e.target.classList.contains('viz-option')) {
          const index = parseInt(e.target.dataset.index);
          setVizMode(index);
        }
      });
    }

    // ============================================
    // Model Selection
    // ============================================
    function selectModel(modelId) {
      const model = MODELS[modelId];
      if (!model) return;

      state.currentModel = model;
      state.vizMode = 0; // Reset viz mode when switching models

      // Update UI
      document.querySelectorAll('.model-item').forEach(item => {
        item.classList.toggle('active', item.dataset.model === modelId);
      });

      // Update info panel
      elements.infoTitle.textContent = model.name;
      elements.infoContent.innerHTML = `<p>${model.description}</p>`;
      elements.usageContent.innerHTML = `<p>${model.usage}</p>`;
      elements.metricModelSize.textContent = model.modelSize;

      // Render settings
      renderSettings(model);

      // Render viz options
      renderVizOptions(model);

      // Update model status indicator
      updateModelStatusDisplay();

      // If camera is running, load the model for this view
      if (state.isRunning) {
        loadCurrentModel();
      }
    }

    function updateModelStatusDisplay() {
      const modelId = state.currentModel?.id;
      let statusText = state.currentModel?.name || 'None';
      let isLoaded = false;

      // Check if the required model for this view is loaded
      switch (modelId) {
        case 'segmentation':
          isLoaded = !!state.models.segmenter;
          statusText += isLoaded ? ' ‚úì' : ' (not loaded)';
          break;
        case 'depth-estimation':
          isLoaded = !!state.models.depthEstimator;
          statusText += isLoaded ? ' ‚úì' : ' (not loaded)';
          break;
        case 'wall-detection':
        case 'edge-detection':
        case 'stabilization':
        case 'perspective-transform':
          isLoaded = true; // CPU-based, always available
          statusText += ' ‚úì';
          break;
        case 'combined':
          const segLoaded = !!state.models.segmenter;
          statusText += segLoaded ? ' (seg ‚úì)' : ' (seg pending)';
          isLoaded = segLoaded;
          break;
        default:
          isLoaded = true;
      }

      elements.modelStatusText.textContent = statusText;
      elements.modelStatus.classList.remove('active', 'loading', 'error');
      elements.modelStatus.classList.add(isLoaded ? 'active' : 'loading');
    }

    function renderSettings(model) {
      if (!model.settings || model.settings.length === 0) {
        elements.settingsContent.innerHTML = '<p style="font-size: 12px; color: #666;">No settings for this model</p>';
        return;
      }

      elements.settingsContent.innerHTML = model.settings.map(setting => {
        if (setting.type === 'select') {
          return `
            <div class="settings-group">
              <label>${setting.label}</label>
              <select id="setting-${setting.id}" data-setting="${setting.id}">
                ${setting.options.map(opt => `<option value="${opt}" ${opt === setting.default ? 'selected' : ''}>${opt}</option>`).join('')}
              </select>
            </div>
          `;
        } else if (setting.type === 'range') {
          const value = state.settings[setting.id] ?? setting.default;
          return `
            <div class="settings-group">
              <label>${setting.label}</label>
              <input type="range" id="setting-${setting.id}" data-setting="${setting.id}"
                min="${setting.min}" max="${setting.max}" step="${setting.step}" value="${value}">
              <div class="range-value" id="value-${setting.id}">${value}</div>
            </div>
          `;
        } else if (setting.type === 'toggle') {
          const checked = state.settings[setting.id] ?? setting.default;
          return `
            <div class="toggle-row">
              <span class="toggle-label">${setting.label}</span>
              <label class="toggle-switch">
                <input type="checkbox" id="setting-${setting.id}" data-setting="${setting.id}" ${checked ? 'checked' : ''}>
                <span class="toggle-slider"></span>
              </label>
            </div>
          `;
        }
        return '';
      }).join('');

      // Add event listeners for settings
      elements.settingsContent.querySelectorAll('input, select').forEach(input => {
        input.addEventListener('input', (e) => {
          const settingId = e.target.dataset.setting;
          let value = e.target.type === 'checkbox' ? e.target.checked : e.target.value;
          if (e.target.type === 'range') {
            value = parseFloat(value);
            const valueDisplay = document.getElementById(`value-${settingId}`);
            if (valueDisplay) valueDisplay.textContent = value;
          }
          state.settings[settingId] = value;
        });
      });
    }

    function renderVizOptions(model) {
      if (!model.vizOptions || model.vizOptions.length === 0) {
        elements.vizOptions.innerHTML = '<p style="font-size: 12px; color: #666;">No options</p>';
        return;
      }

      elements.vizOptions.innerHTML = model.vizOptions.map((opt, i) => `
        <div class="viz-option ${i === state.vizMode ? 'active' : ''}" data-index="${i}">${opt}</div>
      `).join('');
    }

    function setVizMode(index) {
      state.vizMode = index;
      document.querySelectorAll('.viz-option').forEach((opt, i) => {
        opt.classList.toggle('active', i === index);
      });
    }

    // ============================================
    // Camera
    // ============================================
    async function startCamera() {
      try {
        showLoading('Starting camera...');
        
        state.stream = await navigator.mediaDevices.getUserMedia({
          video: { width: { ideal: 1280 }, height: { ideal: 720 }, facingMode: 'user' }
        });

        elements.webcam.srcObject = state.stream;
        await elements.webcam.play();

        // Set canvas size
        elements.canvas.width = elements.webcam.videoWidth;
        elements.canvas.height = elements.webcam.videoHeight;

        // Update UI
        elements.videoPlaceholder.style.display = 'none';
        elements.cameraStatus.classList.add('active');
        elements.startBtn.disabled = true;
        elements.stopBtn.disabled = false;
        elements.screenshotBtn.disabled = false;

        // Update resolution metric
        elements.metricResolution.textContent = `${elements.webcam.videoWidth}x${elements.webcam.videoHeight}`;

        state.isRunning = true;
        hideLoading();

        // Load model if needed
        await loadCurrentModel();

        // Start render loop
        requestAnimationFrame(renderLoop);

      } catch (error) {
        console.error('Camera error:', error);
        showError('Failed to access camera: ' + error.message);
        hideLoading();
      }
    }

    function stopCamera() {
      state.isRunning = false;

      if (state.animationId) {
        cancelAnimationFrame(state.animationId);
      }

      if (state.stream) {
        state.stream.getTracks().forEach(track => track.stop());
        state.stream = null;
      }

      elements.webcam.srcObject = null;
      elements.videoPlaceholder.style.display = 'flex';
      elements.cameraStatus.classList.remove('active');
      elements.startBtn.disabled = false;
      elements.stopBtn.disabled = true;
      elements.screenshotBtn.disabled = true;

      // Clear canvas
      ctx.clearRect(0, 0, elements.canvas.width, elements.canvas.height);
    }

    // ============================================
    // Model Loading
    // ============================================
    async function loadCurrentModel() {
      if (!state.currentModel) return;

      const modelId = state.currentModel.id;
      console.log(`[Model] Loading model for: ${modelId}`);

      elements.modelStatus.classList.remove('active', 'error');
      elements.modelStatus.classList.add('loading');

      try {
        switch (modelId) {
          case 'segmentation':
            await loadSegmentationModel();
            break;
          case 'depth-estimation':
            await loadDepthModel();
            break;
          case 'wall-detection':
            // Wall detection benefits from both segmentation and depth
            console.log('[Model] Wall detection: loading segmentation...');
            await loadSegmentationModel();
            // Start loading depth in background (don't await - it's slow)
            console.log('[Model] Wall detection: starting depth load in background...');
            loadDepthModel().catch(e => console.warn('[Model] Depth load failed:', e));
            break;
          case 'combined':
            // Combined needs segmentation
            await loadSegmentationModel();
            break;
          case 'perspective-transform':
            // Perspective transform can optionally use segmentation for person occlusion
            console.log('[Model] Perspective transform: loading segmentation for occlusion...');
            await loadSegmentationModel();
            break;
          // Other models don't need async loading
          default:
            console.log(`[Model] ${modelId} is CPU-based, no loading needed`);
        }

        console.log(`[Model] ${modelId} ready`);
        updateModelStatusDisplay();
      } catch (error) {
        console.error('[Model] Load error:', error);
        elements.modelStatus.classList.remove('loading');
        elements.modelStatus.classList.add('error');
        showError('Failed to load model: ' + error.message);
        updateModelStatusDisplay();
      }
    }

    async function loadSegmentationModel() {
      if (state.models.segmenter) return;

      showLoading('Loading MediaPipe Image Segmenter...');

      // Dynamic import MediaPipe Tasks Vision
      const vision = await import('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/+esm');
      
      const { ImageSegmenter, FilesetResolver } = vision;

      const wasmFileset = await FilesetResolver.forVisionTasks(
        'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm'
      );

      state.models.segmenter = await ImageSegmenter.createFromOptions(wasmFileset, {
        baseOptions: {
          modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_segmenter_landscape/float16/latest/selfie_segmenter_landscape.tflite',
          delegate: 'GPU'
        },
        runningMode: 'VIDEO',
        outputCategoryMask: true,
        outputConfidenceMasks: false
      });

      hideLoading();
    }

    async function loadDepthModel() {
      if (state.models.depthEstimator) {
        console.log('[Depth] Model already loaded');
        return;
      }

      const deviceSetting = state.settings['device'] || 'WebGPU (Fast)';
      const device = deviceSetting.includes('WebGPU') ? 'webgpu' : 'wasm';

      console.log(`[Depth] Starting to load Depth Anything V2 with device: ${device}`);
      showLoading(`Loading Depth Anything V2 (${device.toUpperCase()})... This may take a moment on first load.`);

      try {
        // Dynamic import Transformers.js
        console.log('[Depth] Importing Transformers.js...');
        const { pipeline, env } = await import('https://cdn.jsdelivr.net/npm/@huggingface/transformers@3');
        console.log('[Depth] Transformers.js imported successfully');

        // Configure for browser use
        env.allowLocalModels = false;
        env.useBrowserCache = true;

        // Check WebGPU availability
        let actualDevice = device;
        if (device === 'webgpu') {
          if (!navigator.gpu) {
            console.warn('[Depth] WebGPU not available, falling back to WASM');
            actualDevice = 'wasm';
            showLoading('WebGPU not available. Loading with WASM (slower)...');
          } else {
            console.log('[Depth] WebGPU is available');
          }
        }

        // Load the depth estimation pipeline
        console.log(`[Depth] Loading pipeline on ${actualDevice}... (this downloads ~27MB on first run)`);
        state.models.depthEstimator = await pipeline(
          'depth-estimation',
          'onnx-community/depth-anything-v2-small',
          {
            device: actualDevice,
            dtype: actualDevice === 'webgpu' ? 'fp32' : 'q8'
          }
        );

        console.log('[Depth] ‚úì Model loaded successfully on', actualDevice);
        hideLoading();
      } catch (error) {
        console.error('[Depth] Load error:', error);
        hideLoading();
        throw new Error(`Failed to load depth model: ${error.message}`);
      }
    }

    // ============================================
    // Render Loop
    // ============================================
    let lastFrameTime = 0;

    function renderLoop(timestamp) {
      if (!state.isRunning) return;

      state.animationId = requestAnimationFrame(renderLoop);

      // Calculate FPS
      state.frameCount++;
      if (timestamp - state.lastFpsUpdate >= 1000) {
        state.fps = state.frameCount;
        state.frameCount = 0;
        state.lastFpsUpdate = timestamp;
        updateMetrics();
      }

      // Run inference
      const startTime = performance.now();
      processFrame();
      state.inferenceTime = performance.now() - startTime;
    }

    function processFrame() {
      if (!state.currentModel) {
        // Just draw video
        ctx.drawImage(elements.webcam, 0, 0);
        return;
      }

      switch (state.currentModel.id) {
        case 'segmentation':
          processSegmentation();
          break;
        case 'wall-detection':
          processWallDetection();
          break;
        case 'edge-detection':
          processEdgeDetection();
          break;
        case 'depth-estimation':
          processDepthEstimation();
          break;
        case 'stabilization':
          processStabilization();
          break;
        case 'combined':
          processCombined();
          break;
        case 'perspective-transform':
          processPerspectiveTransform();
          break;
        default:
          ctx.drawImage(elements.webcam, 0, 0);
      }
    }

    // ============================================
    // Model Processing Functions
    // ============================================
    function processSegmentation() {
      const segmenter = state.models.segmenter;
      
      if (!segmenter) {
        ctx.drawImage(elements.webcam, 0, 0);
        return;
      }

      // Run segmentation
      const result = segmenter.segmentForVideo(elements.webcam, performance.now());
      
      // Draw base video
      ctx.drawImage(elements.webcam, 0, 0);

      if (result.categoryMask) {
        const mask = result.categoryMask;
        const maskData = mask.getAsUint8Array();
        const width = mask.width;
        const height = mask.height;

        // Create ImageData for mask visualization
        const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
        const pixels = imageData.data;

        // Apply visualization based on mode
        const vizMode = state.currentModel.vizOptions[state.vizMode];

        for (let i = 0; i < maskData.length; i++) {
          const isPerson = maskData[i] > 0;
          const pixelIndex = i * 4;

          if (vizMode === 'Mask Only') {
            pixels[pixelIndex] = isPerson ? 255 : 0;
            pixels[pixelIndex + 1] = isPerson ? 255 : 0;
            pixels[pixelIndex + 2] = isPerson ? 255 : 0;
          } else if (vizMode === 'Colored Overlay') {
            if (isPerson) {
              pixels[pixelIndex] = Math.min(255, pixels[pixelIndex] + 50);
              pixels[pixelIndex + 2] = Math.max(0, pixels[pixelIndex + 2] - 50);
            }
          } else if (vizMode === 'Background Blur') {
            // Simplified blur effect - just darken background
            if (!isPerson) {
              pixels[pixelIndex] = pixels[pixelIndex] * 0.3;
              pixels[pixelIndex + 1] = pixels[pixelIndex + 1] * 0.3;
              pixels[pixelIndex + 2] = pixels[pixelIndex + 2] * 0.3;
            }
          } else if (vizMode === 'Background Replace') {
            if (!isPerson) {
              // Replace with solid color
              pixels[pixelIndex] = 30;
              pixels[pixelIndex + 1] = 30;
              pixels[pixelIndex + 2] = 60;
            }
          }
        }

        ctx.putImageData(imageData, 0, 0);
        mask.close();
      }
    }

    // Cache for polygon-based wall detection
    const wallDetectionCache = {
      frameCount: 0,
      lines: [],           // Detected straight lines
      hLines: [],          // Horizontal lines
      vLines: [],          // Vertical lines
      wallPolygon: null,   // Best wall polygon found
      personMask: null,    // Cached person mask
      backgroundMask: null // Inverted person mask
    };

    function processWallDetection() {
      ctx.drawImage(elements.webcam, 0, 0);

      const updateInterval = state.settings['update-interval'] ?? 2;
      const vizMode = state.currentModel.vizOptions[state.vizMode];

      // Update analysis on interval
      wallDetectionCache.frameCount++;
      if (wallDetectionCache.frameCount >= updateInterval) {
        wallDetectionCache.frameCount = 0;
        updatePolygonWallDetection();
      }

      // Render based on mode
      renderPolygonWallViz(vizMode);
    }

    function updatePolygonWallDetection() {
      const width = elements.canvas.width;
      const height = elements.canvas.height;
      const edgeThreshold = state.settings['edge-threshold'] ?? 40;
      const minLineLengthPct = state.settings['min-line-length'] ?? 15;
      const angleTolerance = state.settings['angle-tolerance'] ?? 10;
      const mergeDistance = state.settings['merge-distance'] ?? 15;

      const minLineLength = Math.min(width, height) * minLineLengthPct / 100;

      // 1. Get person mask if segmenter available
      let personMask = null;
      if (state.models.segmenter) {
        const result = state.models.segmenter.segmentForVideo(elements.webcam, performance.now());
        if (result.categoryMask) {
          personMask = result.categoryMask.getAsUint8Array();
          result.categoryMask.close();
        }
      }
      wallDetectionCache.personMask = personMask;

      // 2. Get image data and detect edges
      const imageData = ctx.getImageData(0, 0, width, height);
      const edges = detectEdgesForLines(imageData, edgeThreshold);

      // 3. Find straight lines using a simplified Hough-like approach
      const allLines = findStraightLines(edges, width, height, minLineLength, angleTolerance);

      // 4. Merge nearby parallel lines
      const mergedLines = mergeParallelLines(allLines, mergeDistance, angleTolerance);

      // 5. Separate into horizontal and vertical lines
      const hLines = [];
      const vLines = [];
      for (const line of mergedLines) {
        const angle = Math.abs(line.angle);
        if (angle < angleTolerance || angle > 180 - angleTolerance) {
          hLines.push(line); // Horizontal
        } else if (Math.abs(angle - 90) < angleTolerance) {
          vLines.push(line); // Vertical
        }
      }

      wallDetectionCache.lines = mergedLines;
      wallDetectionCache.hLines = hLines;
      wallDetectionCache.vLines = vLines;

      // 6. Try to form a wall polygon from H/V lines
      const polygon = formWallPolygon(hLines, vLines, width, height, personMask);
      wallDetectionCache.wallPolygon = polygon;
    }

    // Detect edges suitable for line detection
    function detectEdgesForLines(imageData, threshold) {
      const width = imageData.width;
      const height = imageData.height;
      const data = imageData.data;
      const edges = new Uint8Array(width * height);

      // Convert to grayscale
      const gray = new Uint8Array(width * height);
      for (let i = 0; i < data.length; i += 4) {
        gray[i / 4] = Math.round(data[i] * 0.299 + data[i + 1] * 0.587 + data[i + 2] * 0.114);
      }

      // Sobel edge detection
      for (let y = 1; y < height - 1; y++) {
        for (let x = 1; x < width - 1; x++) {
          const idx = y * width + x;

          // Sobel X
          const gx = -gray[(y - 1) * width + (x - 1)] + gray[(y - 1) * width + (x + 1)]
                   - 2 * gray[y * width + (x - 1)] + 2 * gray[y * width + (x + 1)]
                   - gray[(y + 1) * width + (x - 1)] + gray[(y + 1) * width + (x + 1)];

          // Sobel Y
          const gy = -gray[(y - 1) * width + (x - 1)] - 2 * gray[(y - 1) * width + x] - gray[(y - 1) * width + (x + 1)]
                   + gray[(y + 1) * width + (x - 1)] + 2 * gray[(y + 1) * width + x] + gray[(y + 1) * width + (x + 1)];

          const magnitude = Math.sqrt(gx * gx + gy * gy);
          edges[idx] = magnitude > threshold ? 255 : 0;
        }
      }

      return edges;
    }

    // Find straight lines from edge map
    function findStraightLines(edges, width, height, minLength, angleTolerance) {
      const lines = [];
      const step = 4; // Sample every 4th pixel

      // Scan for horizontal lines
      for (let y = 0; y < height; y += step) {
        let lineStart = -1;
        let lineLength = 0;

        for (let x = 0; x < width; x++) {
          const isEdge = edges[y * width + x] > 0;

          if (isEdge) {
            if (lineStart === -1) lineStart = x;
            lineLength++;
          } else {
            if (lineLength >= minLength) {
              lines.push({
                x1: lineStart, y1: y,
                x2: lineStart + lineLength, y2: y,
                angle: 0,
                length: lineLength
              });
            }
            lineStart = -1;
            lineLength = 0;
          }
        }
        // Check end of row
        if (lineLength >= minLength) {
          lines.push({
            x1: lineStart, y1: y,
            x2: lineStart + lineLength, y2: y,
            angle: 0,
            length: lineLength
          });
        }
      }

      // Scan for vertical lines
      for (let x = 0; x < width; x += step) {
        let lineStart = -1;
        let lineLength = 0;

        for (let y = 0; y < height; y++) {
          const isEdge = edges[y * width + x] > 0;

          if (isEdge) {
            if (lineStart === -1) lineStart = y;
            lineLength++;
          } else {
            if (lineLength >= minLength) {
              lines.push({
                x1: x, y1: lineStart,
                x2: x, y2: lineStart + lineLength,
                angle: 90,
                length: lineLength
              });
            }
            lineStart = -1;
            lineLength = 0;
          }
        }
        // Check end of column
        if (lineLength >= minLength) {
          lines.push({
            x1: x, y1: lineStart,
            x2: x, y2: lineStart + lineLength,
            angle: 90,
            length: lineLength
          });
        }
      }

      return lines;
    }

    // Merge nearby parallel lines
    function mergeParallelLines(lines, mergeDistance, angleTolerance) {
      const merged = [];
      const used = new Set();

      for (let i = 0; i < lines.length; i++) {
        if (used.has(i)) continue;

        const line = lines[i];
        let bestLine = { ...line };

        for (let j = i + 1; j < lines.length; j++) {
          if (used.has(j)) continue;

          const other = lines[j];

          // Check if same orientation
          if (Math.abs(line.angle - other.angle) > angleTolerance) continue;

          // Check if close enough
          const isHorizontal = Math.abs(line.angle) < angleTolerance;
          if (isHorizontal) {
            // For horizontal lines, check Y distance
            if (Math.abs(line.y1 - other.y1) < mergeDistance) {
              // Merge by extending
              bestLine.x1 = Math.min(bestLine.x1, other.x1);
              bestLine.x2 = Math.max(bestLine.x2, other.x2);
              bestLine.y1 = (bestLine.y1 + other.y1) / 2;
              bestLine.y2 = bestLine.y1;
              bestLine.length = bestLine.x2 - bestLine.x1;
              used.add(j);
            }
          } else {
            // For vertical lines, check X distance
            if (Math.abs(line.x1 - other.x1) < mergeDistance) {
              bestLine.y1 = Math.min(bestLine.y1, other.y1);
              bestLine.y2 = Math.max(bestLine.y2, other.y2);
              bestLine.x1 = (bestLine.x1 + other.x1) / 2;
              bestLine.x2 = bestLine.x1;
              bestLine.length = bestLine.y2 - bestLine.y1;
              used.add(j);
            }
          }
        }

        merged.push(bestLine);
        used.add(i);
      }

      return merged;
    }

    // Form a wall polygon from detected H/V lines
    function formWallPolygon(hLines, vLines, width, height, personMask) {
      // Sort lines by position
      const topLines = hLines.filter(l => l.y1 < height * 0.4).sort((a, b) => a.y1 - b.y1);
      const bottomLines = hLines.filter(l => l.y1 > height * 0.6).sort((a, b) => b.y1 - a.y1);
      const leftLines = vLines.filter(l => l.x1 < width * 0.4).sort((a, b) => a.x1 - b.x1);
      const rightLines = vLines.filter(l => l.x1 > width * 0.6).sort((a, b) => b.x1 - a.x1);

      // Try to find the best bounding rectangle
      // Priority: use detected lines, fall back to frame edges

      let top = topLines.length > 0 ? topLines[0].y1 : 0;
      let bottom = bottomLines.length > 0 ? bottomLines[0].y1 : height;
      let left = leftLines.length > 0 ? leftLines[0].x1 : 0;
      let right = rightLines.length > 0 ? rightLines[0].x1 : width;

      // If we have a person mask, adjust polygon to avoid person
      if (personMask) {
        const personBounds = findPersonBoundsFromMask(personMask, width, height);
        if (personBounds) {
          // The polygon should be behind the person
          // Keep the parts of the polygon that are in the background
        }
      }

      // Ensure reasonable size
      if (right - left < width * 0.2 || bottom - top < height * 0.2) {
        return null;
      }

      return {
        points: [
          { x: left, y: top },
          { x: right, y: top },
          { x: right, y: bottom },
          { x: left, y: bottom }
        ],
        bounds: { left, top, right, bottom },
        hasDetectedEdges: {
          top: topLines.length > 0,
          bottom: bottomLines.length > 0,
          left: leftLines.length > 0,
          right: rightLines.length > 0
        }
      };
    }

    function findPersonBoundsFromMask(mask, width, height) {
      let minX = width, maxX = 0, minY = height, maxY = 0;
      let found = false;

      for (let y = 0; y < height; y += 4) {
        for (let x = 0; x < width; x += 4) {
          if (mask[y * width + x] > 0) {
            minX = Math.min(minX, x);
            maxX = Math.max(maxX, x);
            minY = Math.min(minY, y);
            maxY = Math.max(maxY, y);
            found = true;
          }
        }
      }

      return found ? { minX, maxX, minY, maxY, centerX: (minX + maxX) / 2 } : null;
    }

    // Render polygon wall visualization
    function renderPolygonWallViz(vizMode) {
      const { lines, hLines, vLines, wallPolygon, personMask } = wallDetectionCache;
      const width = elements.canvas.width;
      const height = elements.canvas.height;

      if (vizMode === 'Wall Polygon') {
        // Draw the detected wall polygon
        if (wallPolygon) {
          ctx.fillStyle = 'rgba(74, 222, 128, 0.25)';
          ctx.beginPath();
          ctx.moveTo(wallPolygon.points[0].x, wallPolygon.points[0].y);
          for (let i = 1; i < wallPolygon.points.length; i++) {
            ctx.lineTo(wallPolygon.points[i].x, wallPolygon.points[i].y);
          }
          ctx.closePath();
          ctx.fill();

          // Draw edges with different colors based on detection
          ctx.lineWidth = 3;
          const edges = wallPolygon.hasDetectedEdges;

          // Top edge
          ctx.strokeStyle = edges.top ? '#22c55e' : '#fbbf24';
          ctx.beginPath();
          ctx.moveTo(wallPolygon.points[0].x, wallPolygon.points[0].y);
          ctx.lineTo(wallPolygon.points[1].x, wallPolygon.points[1].y);
          ctx.stroke();

          // Right edge
          ctx.strokeStyle = edges.right ? '#22c55e' : '#fbbf24';
          ctx.beginPath();
          ctx.moveTo(wallPolygon.points[1].x, wallPolygon.points[1].y);
          ctx.lineTo(wallPolygon.points[2].x, wallPolygon.points[2].y);
          ctx.stroke();

          // Bottom edge
          ctx.strokeStyle = edges.bottom ? '#22c55e' : '#fbbf24';
          ctx.beginPath();
          ctx.moveTo(wallPolygon.points[2].x, wallPolygon.points[2].y);
          ctx.lineTo(wallPolygon.points[3].x, wallPolygon.points[3].y);
          ctx.stroke();

          // Left edge
          ctx.strokeStyle = edges.left ? '#22c55e' : '#fbbf24';
          ctx.beginPath();
          ctx.moveTo(wallPolygon.points[3].x, wallPolygon.points[3].y);
          ctx.lineTo(wallPolygon.points[0].x, wallPolygon.points[0].y);
          ctx.stroke();
        }
      } else if (vizMode === 'All Lines') {
        // Draw all detected lines
        ctx.lineWidth = 2;
        for (const line of lines) {
          const hue = line.angle; // Color by angle
          ctx.strokeStyle = `hsla(${hue}, 80%, 50%, 0.7)`;
          ctx.beginPath();
          ctx.moveTo(line.x1, line.y1);
          ctx.lineTo(line.x2, line.y2);
          ctx.stroke();
        }
      } else if (vizMode === 'H/V Lines') {
        // Draw only horizontal and vertical lines
        ctx.lineWidth = 3;

        // Horizontal lines in blue
        ctx.strokeStyle = 'rgba(59, 130, 246, 0.8)';
        for (const line of hLines) {
          ctx.beginPath();
          ctx.moveTo(line.x1, line.y1);
          ctx.lineTo(line.x2, line.y2);
          ctx.stroke();
        }

        // Vertical lines in green
        ctx.strokeStyle = 'rgba(34, 197, 94, 0.8)';
        for (const line of vLines) {
          ctx.beginPath();
          ctx.moveTo(line.x1, line.y1);
          ctx.lineTo(line.x2, line.y2);
          ctx.stroke();
        }
      } else if (vizMode === 'Background Mask') {
        // Show person mask (foreground in red, background available for wall)
        if (personMask) {
          const imageData = ctx.getImageData(0, 0, width, height);
          const pixels = imageData.data;

          for (let i = 0; i < personMask.length; i++) {
            const isPerson = personMask[i] > 0;
            const idx = i * 4;
            if (isPerson) {
              pixels[idx] = Math.min(255, pixels[idx] + 80);
              pixels[idx + 1] = Math.floor(pixels[idx + 1] * 0.7);
              pixels[idx + 2] = Math.floor(pixels[idx + 2] * 0.7);
            } else {
              pixels[idx + 1] = Math.min(255, pixels[idx + 1] + 40);
            }
          }

          ctx.putImageData(imageData, 0, 0);
        }
      } else if (vizMode === 'Debug') {
        // Show everything
        // Background tint
        if (personMask) {
          const imageData = ctx.getImageData(0, 0, width, height);
          const pixels = imageData.data;
          for (let i = 0; i < personMask.length; i++) {
            if (personMask[i] === 0) {
              pixels[i * 4 + 2] = Math.min(255, pixels[i * 4 + 2] + 30);
            }
          }
          ctx.putImageData(imageData, 0, 0);
        }

        // All lines
        ctx.lineWidth = 1;
        ctx.strokeStyle = 'rgba(255, 255, 0, 0.5)';
        for (const line of lines) {
          ctx.beginPath();
          ctx.moveTo(line.x1, line.y1);
          ctx.lineTo(line.x2, line.y2);
          ctx.stroke();
        }

        // H/V lines thicker
        ctx.lineWidth = 2;
        ctx.strokeStyle = 'rgba(0, 255, 255, 0.8)';
        for (const line of [...hLines, ...vLines]) {
          ctx.beginPath();
          ctx.moveTo(line.x1, line.y1);
          ctx.lineTo(line.x2, line.y2);
          ctx.stroke();
        }

        // Polygon
        if (wallPolygon) {
          ctx.strokeStyle = '#22c55e';
          ctx.lineWidth = 3;
          ctx.beginPath();
          ctx.moveTo(wallPolygon.points[0].x, wallPolygon.points[0].y);
          for (const pt of wallPolygon.points) {
            ctx.lineTo(pt.x, pt.y);
          }
          ctx.closePath();
          ctx.stroke();
        }
      }

      // Info panel
      ctx.fillStyle = 'rgba(0, 0, 0, 0.85)';
      ctx.fillRect(10, height - 100, 250, 90);
      ctx.font = '11px sans-serif';
      ctx.textAlign = 'left';

      ctx.fillStyle = '#4ade80';
      ctx.fillText('Polygon Wall Detection', 20, height - 82);

      ctx.fillStyle = '#888';
      ctx.fillText(`Lines found: ${lines.length} (H: ${hLines.length}, V: ${vLines.length})`, 20, height - 64);
      ctx.fillText(`Polygon: ${wallPolygon ? 'Found' : 'None'}`, 20, height - 46);
      ctx.fillText(`Person mask: ${personMask ? 'Active' : 'None'}`, 20, height - 28);

      // Legend
      if (vizMode === 'Wall Polygon') {
        ctx.fillStyle = '#22c55e';
        ctx.fillText('Green = Detected edge', 140, height - 64);
        ctx.fillStyle = '#fbbf24';
        ctx.fillText('Yellow = Inferred edge', 140, height - 46);
      }
    }

    // ============================================
    // Perspective Transform Demo
    // ============================================
    const perspectiveState = {
      corners: null,  // Will be initialized when model selected
      dragging: null, // Which corner is being dragged ('topLeft', 'topRight', etc.) or 'pan'
      patternCanvas: null,
      loadedImage: null,
      currentPattern: null,
      isLoadingImage: false,
      personMask: null,
      uploadInput: null,
      // Image panning within the quad
      panOffset: { x: 0, y: 0 },
      panStart: null,  // Starting position when pan drag begins
      zoom: 1.0        // Future: allow zoom
    };

    // Sample artwork URLs (public domain from Wikimedia Commons)
    const sampleArtwork = {
      'Starry Night': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/800px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg',
      'Great Wave': 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Tsunami_by_hokusai_19th_century.jpg/800px-Tsunami_by_hokusai_19th_century.jpg',
      'Mona Lisa': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/600px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg'
    };

    function initPerspectiveCorners() {
      const width = elements.canvas.width;
      const height = elements.canvas.height;
      // Default to a slightly inset rectangle
      perspectiveState.corners = {
        topLeft: { x: width * 0.2, y: height * 0.2 },
        topRight: { x: width * 0.8, y: height * 0.2 },
        bottomLeft: { x: width * 0.2, y: height * 0.8 },
        bottomRight: { x: width * 0.8, y: height * 0.8 }
      };
    }

    async function loadSampleArtwork(name) {
      const url = sampleArtwork[name];
      if (!url) return null;

      perspectiveState.isLoadingImage = true;
      console.log(`[Perspective] Loading artwork: ${name}`);

      return new Promise((resolve, reject) => {
        const img = new Image();
        img.crossOrigin = 'anonymous';
        img.onload = () => {
          console.log(`[Perspective] Loaded ${name}: ${img.width}x${img.height}`);
          perspectiveState.isLoadingImage = false;
          resolve(img);
        };
        img.onerror = (e) => {
          console.error(`[Perspective] Failed to load ${name}:`, e);
          perspectiveState.isLoadingImage = false;
          reject(e);
        };
        img.src = url;
      });
    }

    function createUploadInput() {
      if (perspectiveState.uploadInput) return perspectiveState.uploadInput;

      const input = document.createElement('input');
      input.type = 'file';
      input.accept = 'image/*';
      input.style.display = 'none';
      document.body.appendChild(input);

      input.addEventListener('change', async (e) => {
        const file = e.target.files[0];
        if (!file) return;

        const reader = new FileReader();
        reader.onload = (event) => {
          const img = new Image();
          img.onload = () => {
            console.log(`[Perspective] Loaded uploaded image: ${img.width}x${img.height}`);
            perspectiveState.loadedImage = img;
            perspectiveState.currentPattern = 'Upload Image...';
            perspectiveState.patternCanvas = null; // Force regeneration
          };
          img.src = event.target.result;
        };
        reader.readAsDataURL(file);
      });

      perspectiveState.uploadInput = input;
      return input;
    }

    async function getPatternSource(pattern) {
      // Check if we need to load/generate a new pattern
      if (perspectiveState.currentPattern === pattern && perspectiveState.patternCanvas) {
        return perspectiveState.patternCanvas;
      }

      // Handle Upload Image option
      if (pattern === 'Upload Image...') {
        if (perspectiveState.currentPattern !== pattern) {
          // Trigger file upload
          const input = createUploadInput();
          input.click();
          // Return existing pattern while waiting
          if (perspectiveState.patternCanvas) {
            return perspectiveState.patternCanvas;
          }
          // Generate placeholder
          return generatePatternCanvas('Checkerboard', 400, 300);
        }
        // If we have a loaded image from upload, use it
        if (perspectiveState.loadedImage) {
          const canvas = document.createElement('canvas');
          canvas.width = perspectiveState.loadedImage.width;
          canvas.height = perspectiveState.loadedImage.height;
          const ctx = canvas.getContext('2d');
          ctx.drawImage(perspectiveState.loadedImage, 0, 0);
          perspectiveState.patternCanvas = canvas;
          return canvas;
        }
      }

      // Handle sample artwork
      if (sampleArtwork[pattern]) {
        try {
          const img = await loadSampleArtwork(pattern);
          if (img) {
            const canvas = document.createElement('canvas');
            canvas.width = img.width;
            canvas.height = img.height;
            const ctx = canvas.getContext('2d');
            ctx.drawImage(img, 0, 0);
            perspectiveState.patternCanvas = canvas;
            perspectiveState.currentPattern = pattern;
            perspectiveState.loadedImage = img;
            return canvas;
          }
        } catch (e) {
          console.error('[Perspective] Failed to load artwork, falling back to checkerboard');
        }
      }

      // Generate pattern
      const canvas = generatePatternCanvas(pattern, 400, 300);
      perspectiveState.patternCanvas = canvas;
      perspectiveState.currentPattern = pattern;
      return canvas;
    }

    function generatePatternCanvas(pattern, width, height) {
      const canvas = document.createElement('canvas');
      canvas.width = width;
      canvas.height = height;
      const ctx = canvas.getContext('2d');

      if (pattern === 'Checkerboard') {
        const cellSize = Math.max(20, Math.min(width, height) / 8);
        for (let y = 0; y < height; y += cellSize) {
          for (let x = 0; x < width; x += cellSize) {
            const isWhite = ((Math.floor(x / cellSize) + Math.floor(y / cellSize)) % 2) === 0;
            ctx.fillStyle = isWhite ? '#ffffff' : '#2563eb';
            ctx.fillRect(x, y, cellSize, cellSize);
          }
        }
      } else if (pattern === 'Grid Lines') {
        ctx.fillStyle = '#1e3a5f';
        ctx.fillRect(0, 0, width, height);
        ctx.strokeStyle = '#60a5fa';
        ctx.lineWidth = 2;
        const spacing = Math.max(20, Math.min(width, height) / 10);
        for (let x = 0; x <= width; x += spacing) {
          ctx.beginPath();
          ctx.moveTo(x, 0);
          ctx.lineTo(x, height);
          ctx.stroke();
        }
        for (let y = 0; y <= height; y += spacing) {
          ctx.beginPath();
          ctx.moveTo(0, y);
          ctx.lineTo(width, y);
          ctx.stroke();
        }
      } else if (pattern === 'Gradient') {
        const gradient = ctx.createLinearGradient(0, 0, width, height);
        gradient.addColorStop(0, '#f472b6');
        gradient.addColorStop(0.5, '#818cf8');
        gradient.addColorStop(1, '#34d399');
        ctx.fillStyle = gradient;
        ctx.fillRect(0, 0, width, height);
        // Add corner labels
        ctx.fillStyle = '#000';
        ctx.font = 'bold 24px sans-serif';
        ctx.fillText('TL', 10, 30);
        ctx.fillText('TR', width - 40, 30);
        ctx.fillText('BL', 10, height - 10);
        ctx.fillText('BR', width - 40, height - 10);
      } else {
        // Default to checkerboard for unknown patterns
        return generatePatternCanvas('Checkerboard', width, height);
      }

      return canvas;
    }

    function processPerspectiveTransform() {
      // Draw webcam first
      ctx.drawImage(elements.webcam, 0, 0);

      // Initialize corners if needed
      if (!perspectiveState.corners) {
        initPerspectiveCorners();
      }

      // Get pattern/image source
      const pattern = state.settings['pattern'] || 'Starry Night';

      // Handle pattern changes - trigger async load if needed
      if (perspectiveState.currentPattern !== pattern && !perspectiveState.isLoadingImage) {
        // Start loading new pattern
        getPatternSource(pattern).then(canvas => {
          perspectiveState.patternCanvas = canvas;
        }).catch(e => {
          console.error('[Perspective] Pattern load error:', e);
        });
      }

      // Use checkerboard as fallback while loading
      if (!perspectiveState.patternCanvas) {
        perspectiveState.patternCanvas = generatePatternCanvas('Checkerboard', 400, 300);
      }

      // Get person mask if segmentation enabled
      const useSegmentation = state.settings['use-segmentation'] ?? true;
      let personMask = null;
      let maskWidth = 0, maskHeight = 0;
      if (useSegmentation && state.models.segmenter) {
        const result = state.models.segmenter.segmentForVideo(elements.webcam, performance.now());
        if (result.categoryMask) {
          // Get mask dimensions before getting array
          maskWidth = result.categoryMask.width;
          maskHeight = result.categoryMask.height;
          personMask = result.categoryMask.getAsUint8Array();
          result.categoryMask.close();
        }
      }

      const vizMode = state.currentModel.vizOptions[state.vizMode];
      const subdivisions = state.settings['subdivisions'] || 8;
      const opacity = state.settings['opacity'] ?? 0.8;
      const showGrid = state.settings['show-grid'] ?? true;

      const corners = perspectiveState.corners;
      const source = perspectiveState.patternCanvas;

      if (vizMode === 'Transformed Image' || vizMode === 'Before/After') {
        // Create temp canvas for the transformed image
        const tempCanvas = document.createElement('canvas');
        tempCanvas.width = elements.canvas.width;
        tempCanvas.height = elements.canvas.height;
        const tempCtx = tempCanvas.getContext('2d');

        // Draw perspective transformed image
        drawPerspectiveQuadDemo(tempCtx, source, corners, subdivisions);

        // Apply person mask if available (cuts out person so they appear in front)
        if (personMask) {
          applyPersonMaskDemo(tempCtx, personMask, maskWidth, maskHeight);
        }

        // Composite onto main canvas
        ctx.globalAlpha = opacity;
        ctx.drawImage(tempCanvas, 0, 0);
        ctx.globalAlpha = 1;

        // For before/after, show original on left side
        if (vizMode === 'Before/After') {
          const midX = elements.canvas.width / 2;
          ctx.save();
          ctx.beginPath();
          ctx.rect(0, 0, midX, elements.canvas.height);
          ctx.clip();
          ctx.drawImage(elements.webcam, 0, 0);
          // Draw the source image unwarped in top-left
          ctx.globalAlpha = 0.9;
          ctx.drawImage(source, 20, 20, 150, 112);
          ctx.globalAlpha = 1;
          ctx.strokeStyle = '#fff';
          ctx.lineWidth = 2;
          ctx.strokeRect(20, 20, 150, 112);
          ctx.restore();

          // Draw divider
          ctx.strokeStyle = '#fff';
          ctx.lineWidth = 3;
          ctx.beginPath();
          ctx.moveTo(midX, 0);
          ctx.lineTo(midX, elements.canvas.height);
          ctx.stroke();

          ctx.fillStyle = '#fff';
          ctx.font = 'bold 14px sans-serif';
          ctx.fillText('ORIGINAL', 20, elements.canvas.height - 20);
          ctx.fillText('TRANSFORMED', midX + 20, elements.canvas.height - 20);
        }
      } else if (vizMode === 'Wireframe') {
        // Draw just the wireframe mesh
        drawWireframeMesh(ctx, corners, subdivisions);
      } else if (vizMode === 'UV Mapping') {
        // Show UV coordinates as colors
        drawUVMapping(ctx, corners, subdivisions);
      }

      // Draw corner handles and grid overlay
      if (showGrid && vizMode !== 'Wireframe') {
        drawCornerHandles(ctx, corners);
      }

      // Info panel
      drawPerspectiveInfo(ctx, corners);
    }

    function drawPerspectiveQuadDemo(ctx, source, corners, subdivisions) {
      const { topLeft, topRight, bottomLeft, bottomRight } = corners;
      const srcWidth = source.width;
      const srcHeight = source.height;

      // Get pan offset (as percentage of source dimensions)
      const panX = perspectiveState.panOffset.x;
      const panY = perspectiveState.panOffset.y;

      for (let row = 0; row < subdivisions; row++) {
        for (let col = 0; col < subdivisions; col++) {
          const u0 = col / subdivisions;
          const v0 = row / subdivisions;
          const u1 = (col + 1) / subdivisions;
          const v1 = (row + 1) / subdivisions;

          // Source coordinates with pan offset
          // Pan is in pixels, we apply it to shift which part of the image is shown
          const sx0 = u0 * srcWidth + panX;
          const sy0 = v0 * srcHeight + panY;
          const sx1 = u1 * srcWidth + panX;
          const sy1 = v1 * srcHeight + panY;

          // Skip if completely outside source bounds
          if (sx1 < 0 || sy1 < 0 || sx0 > srcWidth || sy0 > srcHeight) {
            continue;
          }

          // Destination coordinates via bilinear interpolation
          const d00 = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, u0, v0);
          const d10 = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, u1, v0);
          const d01 = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, u0, v1);
          const d11 = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, u1, v1);

          // Draw two triangles
          drawTexturedTriangleDemo(ctx, source,
            sx0, sy0, sx1, sy0, sx0, sy1,
            d00.x, d00.y, d10.x, d10.y, d01.x, d01.y
          );
          drawTexturedTriangleDemo(ctx, source,
            sx1, sy0, sx1, sy1, sx0, sy1,
            d10.x, d10.y, d11.x, d11.y, d01.x, d01.y
          );
        }
      }
    }

    function bilinearInterpolateDemo(tl, tr, bl, br, u, v) {
      const top = {
        x: tl.x + (tr.x - tl.x) * u,
        y: tl.y + (tr.y - tl.y) * u
      };
      const bottom = {
        x: bl.x + (br.x - bl.x) * u,
        y: bl.y + (br.y - bl.y) * u
      };
      return {
        x: top.x + (bottom.x - top.x) * v,
        y: top.y + (bottom.y - top.y) * v
      };
    }

    function drawTexturedTriangleDemo(ctx, source, sx0, sy0, sx1, sy1, sx2, sy2, dx0, dy0, dx1, dy1, dx2, dy2) {
      ctx.save();

      ctx.beginPath();
      ctx.moveTo(dx0, dy0);
      ctx.lineTo(dx1, dy1);
      ctx.lineTo(dx2, dy2);
      ctx.closePath();
      ctx.clip();

      const denom = (sx0 * (sy2 - sy1) - sx1 * sy2 + sx2 * sy1 + (sx1 - sx2) * sy0);
      if (Math.abs(denom) < 0.001) {
        ctx.restore();
        return;
      }

      const m11 = -(sy0 * (dx2 - dx1) - sy1 * dx2 + sy2 * dx1 + (sy1 - sy2) * dx0) / denom;
      const m12 = (sy0 * (dy2 - dy1) - sy1 * dy2 + sy2 * dy1 + (sy1 - sy2) * dy0) / denom;
      const m21 = (sx0 * (dx2 - dx1) - sx1 * dx2 + sx2 * dx1 + (sx1 - sx2) * dx0) / denom;
      const m22 = -(sx0 * (dy2 - dy1) - sx1 * dy2 + sy2 * dy1 + (sx1 - sx2) * dy0) / denom;
      const m31 = (sx0 * (sy2 * dx1 - sy1 * dx2) + sy0 * (sx1 * dx2 - sx2 * dx1) + (sx2 * sy1 - sx1 * sy2) * dx0) / denom;
      const m32 = -(sx0 * (sy2 * dy1 - sy1 * dy2) + sy0 * (sx1 * dy2 - sx2 * dy1) + (sx2 * sy1 - sx1 * sy2) * dy0) / denom;

      ctx.transform(m11, m12, m21, m22, m31, m32);
      ctx.drawImage(source, 0, 0);

      ctx.restore();
    }

    function applyPersonMaskDemo(ctx, mask, maskWidth, maskHeight) {
      const canvas = ctx.canvas;
      const canvasWidth = canvas.width;
      const canvasHeight = canvas.height;

      // Use provided dimensions or assume same as canvas
      const mWidth = maskWidth || canvasWidth;
      const mHeight = maskHeight || canvasHeight;

      // Debug: log mask info once
      if (!applyPersonMaskDemo._logged) {
        console.log(`[Perspective] Mask: ${mask.length} values, dimensions: ${mWidth}x${mHeight}`);
        console.log(`[Perspective] Canvas: ${canvasWidth}x${canvasHeight}`);
        // Sample values
        const sample = [...new Set(mask.slice(0, 1000))].sort((a,b) => a-b);
        console.log('[Perspective] Unique mask values (sample):', sample);
        applyPersonMaskDemo._logged = true;
      }

      // Create mask canvas at MASK dimensions
      const maskCanvas = document.createElement('canvas');
      maskCanvas.width = mWidth;
      maskCanvas.height = mHeight;
      const maskCtx = maskCanvas.getContext('2d');

      // Convert category mask (Uint8Array of 0/1) to ImageData (RGBA)
      const maskImageData = maskCtx.createImageData(mWidth, mHeight);

      for (let i = 0; i < mask.length; i++) {
        // MediaPipe Tasks Vision: 0 = background, 1 = person
        // We want art on BACKGROUND, not on person
        // So make BACKGROUND opaque (keep art), PERSON transparent (remove art)
        const isBackground = mask[i] === 0;

        maskImageData.data[i * 4] = 255;       // R
        maskImageData.data[i * 4 + 1] = 255;   // G
        maskImageData.data[i * 4 + 2] = 255;   // B
        maskImageData.data[i * 4 + 3] = isBackground ? 255 : 0; // A - background opaque, person transparent
      }
      maskCtx.putImageData(maskImageData, 0, 0);

      // Blur for soft edges (feathering around person outline)
      maskCtx.filter = 'blur(4px)';
      maskCtx.drawImage(maskCanvas, 0, 0);
      maskCtx.filter = 'none';

      // destination-in: KEEPS destination where source is opaque
      // Art is kept where background is (mask opaque), removed where person is (mask transparent)
      ctx.globalCompositeOperation = 'destination-in';
      ctx.drawImage(maskCanvas, 0, 0, canvasWidth, canvasHeight);
      ctx.globalCompositeOperation = 'source-over';
    }

    function drawWireframeMesh(ctx, corners, subdivisions) {
      const { topLeft, topRight, bottomLeft, bottomRight } = corners;

      ctx.strokeStyle = '#00ff00';
      ctx.lineWidth = 1;

      for (let row = 0; row <= subdivisions; row++) {
        const v = row / subdivisions;
        ctx.beginPath();
        for (let col = 0; col <= subdivisions; col++) {
          const u = col / subdivisions;
          const pt = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, u, v);
          if (col === 0) {
            ctx.moveTo(pt.x, pt.y);
          } else {
            ctx.lineTo(pt.x, pt.y);
          }
        }
        ctx.stroke();
      }

      for (let col = 0; col <= subdivisions; col++) {
        const u = col / subdivisions;
        ctx.beginPath();
        for (let row = 0; row <= subdivisions; row++) {
          const v = row / subdivisions;
          const pt = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, u, v);
          if (row === 0) {
            ctx.moveTo(pt.x, pt.y);
          } else {
            ctx.lineTo(pt.x, pt.y);
          }
        }
        ctx.stroke();
      }

      // Draw border thicker
      ctx.strokeStyle = '#ff0';
      ctx.lineWidth = 3;
      ctx.beginPath();
      ctx.moveTo(topLeft.x, topLeft.y);
      ctx.lineTo(topRight.x, topRight.y);
      ctx.lineTo(bottomRight.x, bottomRight.y);
      ctx.lineTo(bottomLeft.x, bottomLeft.y);
      ctx.closePath();
      ctx.stroke();
    }

    function drawUVMapping(ctx, corners, subdivisions) {
      const { topLeft, topRight, bottomLeft, bottomRight } = corners;

      for (let row = 0; row < subdivisions; row++) {
        for (let col = 0; col < subdivisions; col++) {
          const u = (col + 0.5) / subdivisions;
          const v = (row + 0.5) / subdivisions;

          const d00 = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, col / subdivisions, row / subdivisions);
          const d11 = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, (col + 1) / subdivisions, (row + 1) / subdivisions);

          // Color based on UV: R=U, G=V, B=0
          const r = Math.floor(u * 255);
          const g = Math.floor(v * 255);
          ctx.fillStyle = `rgb(${r}, ${g}, 100)`;

          ctx.beginPath();
          ctx.moveTo(d00.x, d00.y);
          const d10 = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, (col + 1) / subdivisions, row / subdivisions);
          const d01 = bilinearInterpolateDemo(topLeft, topRight, bottomLeft, bottomRight, col / subdivisions, (row + 1) / subdivisions);
          ctx.lineTo(d10.x, d10.y);
          ctx.lineTo(d11.x, d11.y);
          ctx.lineTo(d01.x, d01.y);
          ctx.closePath();
          ctx.fill();
        }
      }
    }

    function drawCornerHandles(ctx, corners) {
      const handleRadius = 12;

      // Draw quad outline
      ctx.strokeStyle = 'rgba(255, 255, 255, 0.8)';
      ctx.lineWidth = 2;
      ctx.setLineDash([5, 5]);
      ctx.beginPath();
      ctx.moveTo(corners.topLeft.x, corners.topLeft.y);
      ctx.lineTo(corners.topRight.x, corners.topRight.y);
      ctx.lineTo(corners.bottomRight.x, corners.bottomRight.y);
      ctx.lineTo(corners.bottomLeft.x, corners.bottomLeft.y);
      ctx.closePath();
      ctx.stroke();
      ctx.setLineDash([]);

      // Draw corner handles
      const cornerNames = ['topLeft', 'topRight', 'bottomLeft', 'bottomRight'];
      const labels = ['TL', 'TR', 'BL', 'BR'];
      const colors = ['#f472b6', '#818cf8', '#34d399', '#fbbf24'];

      cornerNames.forEach((name, i) => {
        const corner = corners[name];
        const isActive = perspectiveState.dragging === name;

        ctx.beginPath();
        ctx.arc(corner.x, corner.y, handleRadius, 0, Math.PI * 2);
        ctx.fillStyle = isActive ? '#fff' : colors[i];
        ctx.fill();
        ctx.strokeStyle = '#000';
        ctx.lineWidth = 2;
        ctx.stroke();

        ctx.fillStyle = isActive ? '#000' : '#fff';
        ctx.font = 'bold 10px sans-serif';
        ctx.textAlign = 'center';
        ctx.textBaseline = 'middle';
        ctx.fillText(labels[i], corner.x, corner.y);
      });
    }

    function drawPerspectiveInfo(ctx, corners) {
      const width = elements.canvas.width;
      const height = elements.canvas.height;
      const pattern = state.settings['pattern'] || 'Starry Night';
      const imgWidth = perspectiveState.patternCanvas?.width || 0;
      const imgHeight = perspectiveState.patternCanvas?.height || 0;
      const panX = Math.round(perspectiveState.panOffset.x);
      const panY = Math.round(perspectiveState.panOffset.y);

      ctx.fillStyle = 'rgba(0, 0, 0, 0.85)';
      ctx.fillRect(10, height - 130, 280, 120);
      ctx.font = '11px sans-serif';
      ctx.textAlign = 'left';

      ctx.fillStyle = '#818cf8';
      ctx.fillText('Perspective Transform Demo', 20, height - 112);

      // Show current image
      ctx.fillStyle = perspectiveState.isLoadingImage ? '#fbbf24' : '#4ade80';
      const statusText = perspectiveState.isLoadingImage ? 'Loading...' : `${pattern} (${imgWidth}x${imgHeight})`;
      ctx.fillText(statusText, 20, height - 95);

      // Show pan offset
      ctx.fillStyle = '#60a5fa';
      ctx.fillText(`Pan: (${panX}, ${panY})`, 200, height - 95);

      ctx.fillStyle = '#888';
      ctx.fillText('Corners: drag handles | Image: drag inside quad', 20, height - 78);

      // Show coordinates
      ctx.fillStyle = '#f472b6';
      ctx.fillText(`TL: (${Math.round(corners.topLeft.x)}, ${Math.round(corners.topLeft.y)})`, 20, height - 58);
      ctx.fillStyle = '#818cf8';
      ctx.fillText(`TR: (${Math.round(corners.topRight.x)}, ${Math.round(corners.topRight.y)})`, 150, height - 58);
      ctx.fillStyle = '#34d399';
      ctx.fillText(`BL: (${Math.round(corners.bottomLeft.x)}, ${Math.round(corners.bottomLeft.y)})`, 20, height - 41);
      ctx.fillStyle = '#fbbf24';
      ctx.fillText(`BR: (${Math.round(corners.bottomRight.x)}, ${Math.round(corners.bottomRight.y)})`, 150, height - 41);

      // Segmentation status
      const segEnabled = state.settings['use-segmentation'] ?? true;
      const segReady = !!state.models.segmenter;
      ctx.fillStyle = segEnabled && segReady ? '#4ade80' : (segEnabled ? '#fbbf24' : '#666');
      ctx.fillText(segEnabled ? (segReady ? '‚úì Person occlusion active' : '‚è≥ Loading segmentation...') : 'Person occlusion disabled', 20, height - 24);

      // Instructions
      ctx.fillStyle = '#666';
      ctx.font = '10px sans-serif';
      ctx.fillText('Settings panel ‚Üí change image, toggle occlusion', 20, height - 10);
    }

    // Check if point is inside quadrilateral using cross product method
    function isPointInQuad(point, corners) {
      const { topLeft, topRight, bottomRight, bottomLeft } = corners;
      const pts = [topLeft, topRight, bottomRight, bottomLeft];

      // Check if point is on the same side of all edges
      let sign = null;
      for (let i = 0; i < 4; i++) {
        const p1 = pts[i];
        const p2 = pts[(i + 1) % 4];
        const cross = (p2.x - p1.x) * (point.y - p1.y) - (p2.y - p1.y) * (point.x - p1.x);
        const s = Math.sign(cross);
        if (sign === null) sign = s;
        else if (s !== 0 && s !== sign) return false;
      }
      return true;
    }

    // Mouse handlers for dragging corners and panning
    function setupPerspectiveMouseHandlers() {
      const canvas = elements.canvas;
      const handleRadius = 15;

      canvas.addEventListener('mousedown', (e) => {
        if (state.currentModel?.id !== 'perspective-transform') return;
        if (!perspectiveState.corners) return;

        const rect = canvas.getBoundingClientRect();
        const scaleX = canvas.width / rect.width;
        const scaleY = canvas.height / rect.height;
        const x = (e.clientX - rect.left) * scaleX;
        const y = (e.clientY - rect.top) * scaleY;

        // Check if clicking on a corner handle first (priority)
        const cornerNames = ['topLeft', 'topRight', 'bottomLeft', 'bottomRight'];
        for (const name of cornerNames) {
          const corner = perspectiveState.corners[name];
          const dx = x - corner.x;
          const dy = y - corner.y;
          if (dx * dx + dy * dy < handleRadius * handleRadius) {
            perspectiveState.dragging = name;
            canvas.style.cursor = 'grabbing';
            e.preventDefault();
            return;
          }
        }

        // Check if clicking inside the quad for panning
        if (isPointInQuad({ x, y }, perspectiveState.corners)) {
          perspectiveState.dragging = 'pan';
          perspectiveState.panStart = { x, y, offsetX: perspectiveState.panOffset.x, offsetY: perspectiveState.panOffset.y };
          canvas.style.cursor = 'move';
          e.preventDefault();
        }
      });

      canvas.addEventListener('mousemove', (e) => {
        if (state.currentModel?.id !== 'perspective-transform') return;
        if (!perspectiveState.corners) return;

        const rect = canvas.getBoundingClientRect();
        const scaleX = canvas.width / rect.width;
        const scaleY = canvas.height / rect.height;
        const x = (e.clientX - rect.left) * scaleX;
        const y = (e.clientY - rect.top) * scaleY;

        if (perspectiveState.dragging === 'pan' && perspectiveState.panStart) {
          // Pan the image within the quad
          const dx = x - perspectiveState.panStart.x;
          const dy = y - perspectiveState.panStart.y;
          // Invert because moving mouse right should show more of left side of image
          perspectiveState.panOffset.x = perspectiveState.panStart.offsetX - dx * 0.5;
          perspectiveState.panOffset.y = perspectiveState.panStart.offsetY - dy * 0.5;
        } else if (perspectiveState.dragging && perspectiveState.dragging !== 'pan') {
          // Move the corner
          perspectiveState.corners[perspectiveState.dragging] = {
            x: Math.max(0, Math.min(canvas.width, x)),
            y: Math.max(0, Math.min(canvas.height, y))
          };
        } else {
          // Check hover state
          const cornerNames = ['topLeft', 'topRight', 'bottomLeft', 'bottomRight'];
          let hoveringCorner = false;
          for (const name of cornerNames) {
            const corner = perspectiveState.corners[name];
            const dx = x - corner.x;
            const dy = y - corner.y;
            if (dx * dx + dy * dy < handleRadius * handleRadius) {
              hoveringCorner = true;
              break;
            }
          }

          if (hoveringCorner) {
            canvas.style.cursor = 'grab';
          } else if (isPointInQuad({ x, y }, perspectiveState.corners)) {
            canvas.style.cursor = 'move';
          } else {
            canvas.style.cursor = 'default';
          }
        }
      });

      canvas.addEventListener('mouseup', () => {
        if (perspectiveState.dragging) {
          perspectiveState.dragging = null;
          perspectiveState.panStart = null;
          elements.canvas.style.cursor = 'default';
        }
      });

      canvas.addEventListener('mouseleave', () => {
        if (perspectiveState.dragging) {
          perspectiveState.dragging = null;
          perspectiveState.panStart = null;
          elements.canvas.style.cursor = 'default';
        }
      });
    }

    // Initialize mouse handlers
    setupPerspectiveMouseHandlers();

    function processEdgeDetection() {
      ctx.drawImage(elements.webcam, 0, 0);

      const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
      const threshold = state.settings['threshold'] || 40;
      const minLength = state.settings['min-length'] || 15;

      // Simple Sobel edge detection
      const edges = detectEdgesSobel(imageData, threshold);

      const vizMode = state.currentModel.vizOptions[state.vizMode];

      ctx.strokeStyle = '#e94560';
      ctx.lineWidth = 2;

      edges.forEach(edge => {
        if (vizMode === 'Strong Edges Only' && edge.strength < threshold * 1.5) return;
        
        ctx.beginPath();
        ctx.moveTo(edge.x1, edge.y1);
        ctx.lineTo(edge.x2, edge.y2);
        
        if (vizMode === 'Edge Angles') {
          // Color by angle
          const hue = (edge.angle + 180) / 360 * 255;
          ctx.strokeStyle = `hsl(${hue}, 80%, 60%)`;
        }
        
        ctx.stroke();
      });
    }

    function detectEdgesSobel(imageData, threshold) {
      const width = imageData.width;
      const height = imageData.height;
      const data = imageData.data;
      const edges = [];

      // Convert to grayscale and apply Sobel
      const gray = new Float32Array(width * height);
      for (let i = 0; i < data.length; i += 4) {
        gray[i / 4] = (data[i] * 0.299 + data[i + 1] * 0.587 + data[i + 2] * 0.114);
      }

      // Sobel kernels
      const sobelX = [-1, 0, 1, -2, 0, 2, -1, 0, 1];
      const sobelY = [-1, -2, -1, 0, 0, 0, 1, 2, 1];

      // Find edge points
      const step = 8; // Sample every 8 pixels
      for (let y = 1; y < height - 1; y += step) {
        for (let x = 1; x < width - 1; x += step) {
          let gx = 0, gy = 0;
          
          for (let ky = -1; ky <= 1; ky++) {
            for (let kx = -1; kx <= 1; kx++) {
              const idx = (y + ky) * width + (x + kx);
              const kidx = (ky + 1) * 3 + (kx + 1);
              gx += gray[idx] * sobelX[kidx];
              gy += gray[idx] * sobelY[kidx];
            }
          }

          const magnitude = Math.sqrt(gx * gx + gy * gy);
          
          if (magnitude > threshold) {
            const angle = Math.atan2(gy, gx) * 180 / Math.PI;
            const length = 15;
            const dx = Math.cos(angle * Math.PI / 180) * length;
            const dy = Math.sin(angle * Math.PI / 180) * length;
            
            edges.push({
              x1: x - dx / 2,
              y1: y - dy / 2,
              x2: x + dx / 2,
              y2: y + dy / 2,
              strength: magnitude,
              angle: angle
            });
          }
        }
      }

      return edges;
    }

    // Cache for depth estimation results
    const depthCache = {
      frameCount: 0,
      depthMap: null,
      depthWidth: 0,
      depthHeight: 0,
      isProcessing: false,
      lastInferenceTime: 0
    };

    function processDepthEstimation() {
      ctx.drawImage(elements.webcam, 0, 0);

      const depthEstimator = state.models.depthEstimator;
      const updateInterval = state.settings['update-interval'] ?? 15;
      const vizMode = state.currentModel.vizOptions[state.vizMode];

      // Check if we should update the depth map
      depthCache.frameCount++;
      const shouldUpdate = (depthCache.frameCount >= updateInterval || !depthCache.depthMap)
                           && !depthCache.isProcessing
                           && depthEstimator;

      if (shouldUpdate) {
        depthCache.frameCount = 0;
        runDepthEstimation();
      }

      // Render using cached depth map
      if (depthCache.depthMap) {
        renderDepthVisualization(vizMode);
      } else {
        // Show status message
        ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
        ctx.fillRect(10, elements.canvas.height - 80, 350, 70);
        ctx.font = '12px sans-serif';
        ctx.textAlign = 'left';

        if (!depthEstimator) {
          ctx.fillStyle = '#fbbf24';
          ctx.fillText('‚è≥ Loading Depth Anything V2 model...', 20, elements.canvas.height - 58);
          ctx.fillStyle = '#888';
          ctx.fillText('First load downloads ~27MB model (cached after)', 20, elements.canvas.height - 40);
          ctx.fillText('Check browser console for progress', 20, elements.canvas.height - 22);
        } else if (depthCache.isProcessing) {
          ctx.fillStyle = '#60a5fa';
          ctx.fillText('üîÑ Running depth estimation...', 20, elements.canvas.height - 58);
          ctx.fillStyle = '#888';
          ctx.fillText('Model loaded, processing first frame', 20, elements.canvas.height - 40);
        } else {
          ctx.fillStyle = '#4ade80';
          ctx.fillText('‚úì Model ready, waiting for first result', 20, elements.canvas.height - 58);
        }
      }
    }

    async function runDepthEstimation() {
      const depthEstimator = state.models.depthEstimator;
      if (!depthEstimator || depthCache.isProcessing) return;

      depthCache.isProcessing = true;
      const startTime = performance.now();

      try {
        // Create a scaled-down canvas for faster inference
        const scale = state.settings['resolution-scale'] ?? 0.5;
        const scaledWidth = Math.round(elements.canvas.width * scale);
        const scaledHeight = Math.round(elements.canvas.height * scale);

        // Create canvas for scaling and convert to data URL
        const tempCanvas = document.createElement('canvas');
        tempCanvas.width = scaledWidth;
        tempCanvas.height = scaledHeight;
        const tempCtx = tempCanvas.getContext('2d');
        tempCtx.drawImage(elements.webcam, 0, 0, scaledWidth, scaledHeight);

        // Convert to data URL (Transformers.js accepts URLs and data URLs)
        const dataUrl = tempCanvas.toDataURL('image/jpeg', 0.8);

        // Run depth estimation
        const result = await depthEstimator(dataUrl);

        console.log('Depth result:', result);

        // Extract depth data from result - output is "predicted_depth" tensor
        if (result && result.predicted_depth) {
          const depthTensor = result.predicted_depth;
          depthCache.depthMap = depthTensor.data;
          depthCache.depthWidth = depthTensor.dims[1]; // dims is [height, width]
          depthCache.depthHeight = depthTensor.dims[0];
          console.log('Depth map updated:', depthCache.depthWidth, 'x', depthCache.depthHeight);
        } else {
          console.warn('Unexpected depth result format:', result);
        }

        depthCache.lastInferenceTime = performance.now() - startTime;
      } catch (error) {
        console.error('Depth estimation error:', error);
      } finally {
        depthCache.isProcessing = false;
      }
    }

    function renderDepthVisualization(vizMode) {
      const { depthMap, depthWidth, depthHeight, lastInferenceTime } = depthCache;
      if (!depthMap) return;

      const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
      const pixels = imageData.data;

      // Find min/max for normalization
      let minDepth = Infinity, maxDepth = -Infinity;
      for (let i = 0; i < depthMap.length; i++) {
        minDepth = Math.min(minDepth, depthMap[i]);
        maxDepth = Math.max(maxDepth, depthMap[i]);
      }
      const depthRange = maxDepth - minDepth || 1;

      // Scale factors for mapping depth map to canvas
      const scaleX = depthWidth / elements.canvas.width;
      const scaleY = depthHeight / elements.canvas.height;

      // Wall plane detection for "Wall Planes" viz mode
      let wallPlanes = null;
      if (vizMode === 'Wall Planes') {
        wallPlanes = detectWallPlanes(depthMap, depthWidth, depthHeight, minDepth, maxDepth);
      }

      for (let y = 0; y < elements.canvas.height; y++) {
        for (let x = 0; x < elements.canvas.width; x++) {
          // Sample depth map (nearest neighbor)
          const dx = Math.floor(x * scaleX);
          const dy = Math.floor(y * scaleY);
          const depthIdx = dy * depthWidth + dx;
          const rawDepth = depthMap[depthIdx] || 0;

          // Normalize depth to 0-1 (closer = lower value in Depth Anything)
          const depth = (rawDepth - minDepth) / depthRange;

          const pixelIndex = (y * elements.canvas.width + x) * 4;

          if (vizMode === 'Depth Map') {
            // Color gradient: red (near) to blue (far)
            const hue = depth * 240;
            const rgb = hslToRgb(hue / 360, 0.8, 0.5);
            pixels[pixelIndex] = rgb[0];
            pixels[pixelIndex + 1] = rgb[1];
            pixels[pixelIndex + 2] = rgb[2];
          } else if (vizMode === 'Depth Contours') {
            // Draw contour lines
            const contourInterval = 0.08;
            const contourValue = depth % contourInterval;
            if (contourValue < 0.008 || contourValue > contourInterval - 0.008) {
              pixels[pixelIndex] = 255;
              pixels[pixelIndex + 1] = 255;
              pixels[pixelIndex + 2] = 255;
            }
          } else if (vizMode === 'Near/Far Regions') {
            // Highlight near (person) and far (wall) regions
            if (depth < 0.35) {
              // Near - red tint
              pixels[pixelIndex] = Math.min(255, pixels[pixelIndex] + 100);
              pixels[pixelIndex + 1] = Math.floor(pixels[pixelIndex + 1] * 0.6);
              pixels[pixelIndex + 2] = Math.floor(pixels[pixelIndex + 2] * 0.6);
            } else if (depth > 0.65) {
              // Far - blue tint
              pixels[pixelIndex] = Math.floor(pixels[pixelIndex] * 0.6);
              pixels[pixelIndex + 1] = Math.floor(pixels[pixelIndex + 1] * 0.6);
              pixels[pixelIndex + 2] = Math.min(255, pixels[pixelIndex + 2] + 100);
            }
          } else if (vizMode === 'Wall Planes' && wallPlanes) {
            // Color by wall plane membership
            const planeId = wallPlanes[dy * depthWidth + dx];
            if (planeId > 0) {
              const planeHue = (planeId * 67) % 360; // Spread colors
              const rgb = hslToRgb(planeHue / 360, 0.7, 0.5);
              pixels[pixelIndex] = Math.floor(pixels[pixelIndex] * 0.4 + rgb[0] * 0.6);
              pixels[pixelIndex + 1] = Math.floor(pixels[pixelIndex + 1] * 0.4 + rgb[1] * 0.6);
              pixels[pixelIndex + 2] = Math.floor(pixels[pixelIndex + 2] * 0.4 + rgb[2] * 0.6);
            }
          }
        }
      }

      ctx.putImageData(imageData, 0, 0);

      // Add info overlay
      ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
      ctx.fillRect(10, elements.canvas.height - 80, 300, 70);
      ctx.fillStyle = '#4ade80';
      ctx.font = '12px sans-serif';
      ctx.textAlign = 'left';
      ctx.fillText('Depth Anything V2 Small', 20, elements.canvas.height - 60);
      ctx.fillStyle = '#888';
      ctx.fillText(`Inference: ${lastInferenceTime.toFixed(0)}ms | Resolution: ${depthWidth}x${depthHeight}`, 20, elements.canvas.height - 42);
      ctx.fillText(`Depth range: ${minDepth.toFixed(2)} - ${maxDepth.toFixed(2)}`, 20, elements.canvas.height - 24);
    }

    // Detect flat wall planes from depth map
    function detectWallPlanes(depthMap, width, height, minDepth, maxDepth) {
      const depthRange = maxDepth - minDepth || 1;
      const planes = new Int32Array(width * height);
      const depthTolerance = depthRange * 0.05; // 5% of depth range

      let planeId = 0;
      const gridSize = 8;
      const cellWidth = Math.floor(width / gridSize);
      const cellHeight = Math.floor(height / gridSize);

      // Analyze grid cells for consistent depth
      for (let gy = 0; gy < gridSize; gy++) {
        for (let gx = 0; gx < gridSize; gx++) {
          const startX = gx * cellWidth;
          const startY = gy * cellHeight;

          // Sample depth values in this cell
          let sumDepth = 0, count = 0;
          for (let y = startY; y < startY + cellHeight && y < height; y += 2) {
            for (let x = startX; x < startX + cellWidth && x < width; x += 2) {
              sumDepth += depthMap[y * width + x];
              count++;
            }
          }
          const avgDepth = sumDepth / count;

          // Calculate variance
          let variance = 0;
          for (let y = startY; y < startY + cellHeight && y < height; y += 2) {
            for (let x = startX; x < startX + cellWidth && x < width; x += 2) {
              const diff = depthMap[y * width + x] - avgDepth;
              variance += diff * diff;
            }
          }
          variance = Math.sqrt(variance / count);

          // If low variance (flat surface), mark as wall plane
          if (variance < depthTolerance) {
            planeId++;
            for (let y = startY; y < startY + cellHeight && y < height; y++) {
              for (let x = startX; x < startX + cellWidth && x < width; x++) {
                planes[y * width + x] = planeId;
              }
            }
          }
        }
      }

      return planes;
    }

    // HSL to RGB helper
    function hslToRgb(h, s, l) {
      let r, g, b;
      if (s === 0) {
        r = g = b = l;
      } else {
        const hue2rgb = (p, q, t) => {
          if (t < 0) t += 1;
          if (t > 1) t -= 1;
          if (t < 1/6) return p + (q - p) * 6 * t;
          if (t < 1/2) return q;
          if (t < 2/3) return p + (q - p) * (2/3 - t) * 6;
          return p;
        };
        const q = l < 0.5 ? l * (1 + s) : l + s - l * s;
        const p = 2 * l - q;
        r = hue2rgb(p, q, h + 1/3);
        g = hue2rgb(p, q, h);
        b = hue2rgb(p, q, h - 1/3);
      }
      return [Math.round(r * 255), Math.round(g * 255), Math.round(b * 255)];
    }

    function processStabilization() {
      ctx.drawImage(elements.webcam, 0, 0);

      // Draw a test pattern to demonstrate stabilization
      const mode = state.settings['mode'] || 'Current (Smoothing)';
      const smoothing = state.settings['smoothing'] || 0.7;

      // Simulated "detected" position with jitter
      const baseX = elements.canvas.width / 2;
      const baseY = elements.canvas.height / 2;
      const jitter = 5;
      const rawX = baseX + (Math.random() - 0.5) * jitter * 2;
      const rawY = baseY + (Math.random() - 0.5) * jitter * 2;

      // Apply stabilization based on mode
      let displayX, displayY;
      
      if (!state.stabilization) {
        state.stabilization = { x: baseX, y: baseY, trail: [] };
      }

      if (mode === 'None') {
        displayX = rawX;
        displayY = rawY;
      } else if (mode === 'Current (Smoothing)') {
        state.stabilization.x = state.stabilization.x * smoothing + rawX * (1 - smoothing);
        state.stabilization.y = state.stabilization.y * smoothing + rawY * (1 - smoothing);
        displayX = state.stabilization.x;
        displayY = state.stabilization.y;
      } else {
        // Feature tracking simulation - more stable
        state.stabilization.x = state.stabilization.x * 0.95 + rawX * 0.05;
        state.stabilization.y = state.stabilization.y * 0.95 + rawY * 0.05;
        displayX = state.stabilization.x;
        displayY = state.stabilization.y;
      }

      // Track trail
      state.stabilization.trail.push({ x: displayX, y: displayY });
      if (state.stabilization.trail.length > 60) {
        state.stabilization.trail.shift();
      }

      const vizMode = state.currentModel.vizOptions[state.vizMode];

      // Draw based on viz mode
      if (vizMode === 'Position Trail') {
        // Draw trail
        ctx.beginPath();
        ctx.strokeStyle = 'rgba(233, 69, 96, 0.5)';
        ctx.lineWidth = 2;
        state.stabilization.trail.forEach((pt, i) => {
          if (i === 0) ctx.moveTo(pt.x, pt.y);
          else ctx.lineTo(pt.x, pt.y);
        });
        ctx.stroke();
      }

      // Draw test art (rectangle)
      const artWidth = 200;
      const artHeight = 150;
      ctx.strokeStyle = '#e94560';
      ctx.lineWidth = 3;
      ctx.strokeRect(displayX - artWidth / 2, displayY - artHeight / 2, artWidth, artHeight);
      
      ctx.fillStyle = 'rgba(233, 69, 96, 0.2)';
      ctx.fillRect(displayX - artWidth / 2, displayY - artHeight / 2, artWidth, artHeight);

      // Label
      ctx.fillStyle = '#fff';
      ctx.font = '14px sans-serif';
      ctx.textAlign = 'center';
      ctx.fillText(`Mode: ${mode}`, displayX, displayY);
    }

    function processCombined() {
      const showSegmentation = state.settings['show-segmentation'] ?? true;
      const showWalls = state.settings['show-walls'] ?? true;
      const showEdges = state.settings['show-edges'] ?? false;

      // Draw base video
      ctx.drawImage(elements.webcam, 0, 0);

      const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
      const pixels = imageData.data;

      // 1. Segmentation (if model loaded and enabled)
      let segmentationMask = null;
      if (showSegmentation && state.models.segmenter) {
        const result = state.models.segmenter.segmentForVideo(elements.webcam, performance.now());
        if (result.categoryMask) {
          segmentationMask = result.categoryMask.getAsUint8Array();
          result.categoryMask.close();
        }
      }

      // 2. Wall Detection
      let wallMask = null;
      if (showWalls) {
        wallMask = detectWallsSimple(imageData);
      }

      // Apply combined visualization
      for (let i = 0; i < pixels.length / 4; i++) {
        const pixelIndex = i * 4;
        const isPerson = segmentationMask ? segmentationMask[i] > 0 : false;
        const isWall = wallMask ? wallMask[i] : false;

        if (isPerson) {
          // Person - subtle red tint
          pixels[pixelIndex] = Math.min(255, pixels[pixelIndex] + 30);
        } else if (isWall) {
          // Wall region - green tint
          pixels[pixelIndex + 1] = Math.min(255, pixels[pixelIndex + 1] + 40);
        }
      }

      ctx.putImageData(imageData, 0, 0);

      // 3. Edge Detection overlay
      if (showEdges) {
        const threshold = 40;
        const freshImageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
        const edges = detectEdgesSobel(freshImageData, threshold);

        ctx.strokeStyle = 'rgba(255, 255, 0, 0.6)';
        ctx.lineWidth = 1;
        edges.forEach(edge => {
          ctx.beginPath();
          ctx.moveTo(edge.x1, edge.y1);
          ctx.lineTo(edge.x2, edge.y2);
          ctx.stroke();
        });
      }

      // Draw legend
      ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
      ctx.fillRect(10, 10, 200, 90);
      ctx.font = '12px sans-serif';
      ctx.textAlign = 'left';

      ctx.fillStyle = '#e94560';
      ctx.fillRect(20, 25, 12, 12);
      ctx.fillStyle = '#fff';
      ctx.fillText('Person (Segmentation)', 40, 35);

      ctx.fillStyle = '#4ade80';
      ctx.fillRect(20, 45, 12, 12);
      ctx.fillStyle = '#fff';
      ctx.fillText('Wall Region', 40, 55);

      if (showEdges) {
        ctx.fillStyle = '#fbbf24';
        ctx.fillRect(20, 65, 12, 12);
        ctx.fillStyle = '#fff';
        ctx.fillText('Detected Edges', 40, 75);
      }

      ctx.fillStyle = '#888';
      ctx.font = '10px sans-serif';
      ctx.fillText('Toggle layers in Settings panel', 20, 92);
    }

    // Improved wall detection for combined view
    function detectWallsSimple(imageData) {
      const width = imageData.width;
      const height = imageData.height;
      const data = imageData.data;
      const gridSize = 8;
      const cellWidth = Math.floor(width / gridSize);
      const cellHeight = Math.floor(height / gridSize);
      const wallMask = new Array(width * height).fill(false);

      // Detect ceiling boundary
      const ceilingBoundaryY = detectCeilingBoundarySimple(imageData);

      for (let gy = 0; gy < gridSize; gy++) {
        for (let gx = 0; gx < gridSize; gx++) {
          const startX = gx * cellWidth;
          const startY = gy * cellHeight;
          const centerY = startY + cellHeight / 2;

          // Calculate variance
          let totalR = 0, totalG = 0, totalB = 0, count = 0;
          for (let y = startY; y < startY + cellHeight && y < height; y += 4) {
            for (let x = startX; x < startX + cellWidth && x < width; x += 4) {
              const i = (y * width + x) * 4;
              totalR += data[i];
              totalG += data[i + 1];
              totalB += data[i + 2];
              count++;
            }
          }

          const avgR = totalR / count, avgG = totalG / count, avgB = totalB / count;
          let variance = 0;

          for (let y = startY; y < startY + cellHeight && y < height; y += 4) {
            for (let x = startX; x < startX + cellWidth && x < width; x += 4) {
              const i = (y * width + x) * 4;
              const diffR = data[i] - avgR;
              const diffG = data[i + 1] - avgG;
              const diffB = data[i + 2] - avgB;
              variance += (diffR * diffR + diffG * diffG + diffB * diffB) / 3;
            }
          }
          variance = Math.sqrt(variance / count);

          // Calculate uniformity score
          const uniformityScore = Math.max(0, 1 - variance / 50);

          // Calculate vertical position score (bell curve)
          const normalizedY = gy / gridSize;
          const verticalScore = Math.exp(-Math.pow(normalizedY - 0.5, 2) / (2 * 0.3 * 0.3));

          // Ceiling penalty
          const ceilingPenalty = centerY < ceilingBoundaryY ? 0.3 : 1.0;

          // Combined score
          const wallScore = (uniformityScore * 0.6 + verticalScore * 0.4) * ceilingPenalty;

          // Mark as wall if score is high enough
          if (wallScore > 0.45) {
            for (let y = startY; y < startY + cellHeight && y < height; y++) {
              for (let x = startX; x < startX + cellWidth && x < width; x++) {
                wallMask[y * width + x] = true;
              }
            }
          }
        }
      }

      return wallMask;
    }

    // Simple ceiling boundary detection for combined view
    function detectCeilingBoundarySimple(imageData) {
      const width = imageData.width;
      const height = imageData.height;
      const data = imageData.data;
      const maxY = Math.floor(height * 0.25);

      let strongestEdgeY = 0;
      let strongestEdgeStrength = 0;

      for (let y = 5; y < maxY; y++) {
        let edgeStrength = 0;

        for (let x = 10; x < width - 10; x += 5) {
          const above = ((y - 2) * width + x) * 4;
          const below = ((y + 2) * width + x) * 4;

          const grayAbove = (data[above] + data[above + 1] + data[above + 2]) / 3;
          const grayBelow = (data[below] + data[below + 1] + data[below + 2]) / 3;

          edgeStrength += Math.abs(grayAbove - grayBelow);
        }

        edgeStrength /= Math.floor((width - 20) / 5);

        if (edgeStrength > strongestEdgeStrength && edgeStrength > 15) {
          strongestEdgeStrength = edgeStrength;
          strongestEdgeY = y;
        }
      }

      return strongestEdgeY;
    }

    // ============================================
    // UI Helpers
    // ============================================
    function updateMetrics() {
      elements.fpsDisplay.textContent = `${state.fps} FPS`;
      elements.inferenceDisplay.textContent = `${state.inferenceTime.toFixed(1)} ms`;
      elements.metricFps.textContent = state.fps;
      elements.metricInference.textContent = state.inferenceTime.toFixed(1);
    }

    function showLoading(text = 'Loading...') {
      elements.loadingOverlay.querySelector('.loading-text').textContent = text;
      elements.loadingOverlay.classList.remove('hidden');
    }

    function hideLoading() {
      elements.loadingOverlay.classList.add('hidden');
    }

    function showError(message) {
      elements.errorToast.textContent = message;
      elements.errorToast.classList.add('visible');
      setTimeout(() => {
        elements.errorToast.classList.remove('visible');
      }, 5000);
    }

    function takeScreenshot() {
      const link = document.createElement('a');
      link.download = `vision-playground-${Date.now()}.png`;
      link.href = elements.canvas.toDataURL();
      link.click();
    }

    // ============================================
    // Start
    // ============================================
    init();
  </script>
</body>
</html>
