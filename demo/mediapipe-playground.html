<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MediaPipe & Vision Models Playground</title>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: #1a1a2e;
      color: #eee;
      min-height: 100vh;
    }

    /* Header */
    .header {
      background: #16213e;
      padding: 16px 24px;
      display: flex;
      align-items: center;
      justify-content: space-between;
      border-bottom: 1px solid #0f3460;
    }

    .header h1 {
      font-size: 20px;
      font-weight: 500;
      color: #e94560;
    }

    .header-status {
      display: flex;
      align-items: center;
      gap: 16px;
      font-size: 13px;
    }

    .status-item {
      display: flex;
      align-items: center;
      gap: 6px;
    }

    .status-dot {
      width: 8px;
      height: 8px;
      border-radius: 50%;
      background: #666;
    }

    .status-dot.active {
      background: #4ade80;
    }

    .status-dot.loading {
      background: #fbbf24;
      animation: pulse 1s infinite;
    }

    .status-dot.error {
      background: #ef4444;
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.5; }
    }

    /* Main Layout */
    .main-container {
      display: flex;
      height: calc(100vh - 60px);
    }

    /* Sidebar */
    .sidebar {
      width: 280px;
      background: #16213e;
      border-right: 1px solid #0f3460;
      display: flex;
      flex-direction: column;
      overflow-y: auto;
    }

    .sidebar-section {
      padding: 16px;
      border-bottom: 1px solid #0f3460;
    }

    .sidebar-section h3 {
      font-size: 11px;
      text-transform: uppercase;
      letter-spacing: 1px;
      color: #888;
      margin-bottom: 12px;
    }

    /* Model Selector */
    .model-list {
      display: flex;
      flex-direction: column;
      gap: 4px;
    }

    .model-item {
      display: flex;
      align-items: center;
      padding: 10px 12px;
      border-radius: 6px;
      cursor: pointer;
      transition: background 0.2s;
      gap: 10px;
    }

    .model-item:hover {
      background: rgba(233, 69, 96, 0.1);
    }

    .model-item.active {
      background: rgba(233, 69, 96, 0.2);
      border: 1px solid #e94560;
    }

    .model-item.disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }

    .model-icon {
      width: 32px;
      height: 32px;
      border-radius: 6px;
      background: #0f3460;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 16px;
    }

    .model-info {
      flex: 1;
    }

    .model-name {
      font-size: 13px;
      font-weight: 500;
    }

    .model-status {
      font-size: 11px;
      color: #888;
    }

    .model-badge {
      font-size: 9px;
      padding: 2px 6px;
      border-radius: 3px;
      text-transform: uppercase;
      font-weight: 600;
    }

    .model-badge.current {
      background: #4ade80;
      color: #000;
    }

    .model-badge.potential {
      background: #fbbf24;
      color: #000;
    }

    /* Settings Panel */
    .settings-group {
      margin-bottom: 16px;
    }

    .settings-group label {
      display: block;
      font-size: 12px;
      color: #aaa;
      margin-bottom: 6px;
    }

    .settings-group select,
    .settings-group input[type="range"] {
      width: 100%;
      padding: 8px;
      border-radius: 4px;
      border: 1px solid #0f3460;
      background: #1a1a2e;
      color: #eee;
      font-size: 13px;
    }

    .settings-group input[type="range"] {
      padding: 0;
      height: 6px;
      -webkit-appearance: none;
      background: #0f3460;
    }

    .settings-group input[type="range"]::-webkit-slider-thumb {
      -webkit-appearance: none;
      width: 16px;
      height: 16px;
      border-radius: 50%;
      background: #e94560;
      cursor: pointer;
    }

    .range-value {
      font-size: 12px;
      color: #e94560;
      text-align: right;
      margin-top: 4px;
    }

    .toggle-row {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 8px 0;
    }

    .toggle-label {
      font-size: 13px;
    }

    .toggle-switch {
      position: relative;
      width: 40px;
      height: 22px;
    }

    .toggle-switch input {
      opacity: 0;
      width: 0;
      height: 0;
    }

    .toggle-slider {
      position: absolute;
      cursor: pointer;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: #0f3460;
      border-radius: 22px;
      transition: 0.3s;
    }

    .toggle-slider:before {
      position: absolute;
      content: "";
      height: 16px;
      width: 16px;
      left: 3px;
      bottom: 3px;
      background: #eee;
      border-radius: 50%;
      transition: 0.3s;
    }

    .toggle-switch input:checked + .toggle-slider {
      background: #e94560;
    }

    .toggle-switch input:checked + .toggle-slider:before {
      transform: translateX(18px);
    }

    /* Video Area */
    .video-area {
      flex: 1;
      display: flex;
      flex-direction: column;
      padding: 24px;
      gap: 16px;
    }

    .video-container {
      position: relative;
      flex: 1;
      background: #000;
      border-radius: 12px;
      overflow: hidden;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .video-container video,
    .video-container canvas {
      max-width: 100%;
      max-height: 100%;
      object-fit: contain;
    }

    #webcam {
      position: absolute;
      opacity: 0;
      pointer-events: none;
    }

    #output-canvas {
      position: relative;
      z-index: 1;
    }

    .video-overlay {
      position: absolute;
      top: 16px;
      left: 16px;
      display: flex;
      flex-direction: column;
      gap: 8px;
      z-index: 10;
    }

    .overlay-stat {
      background: rgba(0, 0, 0, 0.7);
      padding: 6px 12px;
      border-radius: 4px;
      font-size: 12px;
      font-family: monospace;
    }

    .overlay-stat.fps {
      color: #4ade80;
    }

    .overlay-stat.inference {
      color: #fbbf24;
    }

    .video-placeholder {
      text-align: center;
      color: #666;
    }

    .video-placeholder-icon {
      font-size: 48px;
      margin-bottom: 16px;
    }

    /* Controls Bar */
    .controls-bar {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 12px;
      padding: 16px;
      background: #16213e;
      border-radius: 8px;
    }

    .control-btn {
      padding: 12px 24px;
      border: none;
      border-radius: 6px;
      font-size: 14px;
      font-weight: 500;
      cursor: pointer;
      transition: all 0.2s;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .control-btn.primary {
      background: #e94560;
      color: white;
    }

    .control-btn.primary:hover {
      background: #d63d56;
    }

    .control-btn.secondary {
      background: #0f3460;
      color: #eee;
    }

    .control-btn.secondary:hover {
      background: #1a4a7a;
    }

    .control-btn:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }

    /* Info Panel */
    .info-panel {
      width: 320px;
      background: #16213e;
      border-left: 1px solid #0f3460;
      display: flex;
      flex-direction: column;
      overflow-y: auto;
    }

    .info-section {
      padding: 16px;
      border-bottom: 1px solid #0f3460;
    }

    .info-section h3 {
      font-size: 14px;
      font-weight: 500;
      margin-bottom: 12px;
      color: #e94560;
    }

    .info-text {
      font-size: 13px;
      line-height: 1.6;
      color: #aaa;
    }

    .info-text p {
      margin-bottom: 12px;
    }

    /* Performance Metrics */
    .metrics-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 12px;
    }

    .metric-card {
      background: #1a1a2e;
      padding: 12px;
      border-radius: 6px;
      text-align: center;
    }

    .metric-value {
      font-size: 24px;
      font-weight: 600;
      color: #e94560;
    }

    .metric-label {
      font-size: 11px;
      color: #888;
      margin-top: 4px;
    }

    /* Visualization Options */
    .viz-options {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
    }

    .viz-option {
      padding: 6px 12px;
      border-radius: 4px;
      font-size: 12px;
      background: #1a1a2e;
      border: 1px solid #0f3460;
      cursor: pointer;
      transition: all 0.2s;
    }

    .viz-option:hover {
      border-color: #e94560;
    }

    .viz-option.active {
      background: #e94560;
      border-color: #e94560;
      color: white;
    }

    /* Loading Overlay */
    .loading-overlay {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: rgba(26, 26, 46, 0.95);
      display: flex;
      align-items: center;
      justify-content: center;
      flex-direction: column;
      gap: 24px;
      z-index: 1000;
    }

    .loading-overlay.hidden {
      display: none;
    }

    .loading-spinner {
      width: 48px;
      height: 48px;
      border: 3px solid #0f3460;
      border-top-color: #e94560;
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }

    @keyframes spin {
      to { transform: rotate(360deg); }
    }

    .loading-text {
      font-size: 14px;
      color: #888;
    }

    /* Error Toast */
    .error-toast {
      position: fixed;
      bottom: 24px;
      left: 50%;
      transform: translateX(-50%);
      background: #ef4444;
      color: white;
      padding: 12px 24px;
      border-radius: 8px;
      font-size: 14px;
      z-index: 1001;
      display: none;
    }

    .error-toast.visible {
      display: block;
    }
  </style>
</head>
<body>
  <!-- Loading Overlay -->
  <div id="loading-overlay" class="loading-overlay">
    <div class="loading-spinner"></div>
    <div class="loading-text">Loading models...</div>
  </div>

  <!-- Error Toast -->
  <div id="error-toast" class="error-toast"></div>

  <!-- Header -->
  <header class="header">
    <h1>Vision Models Playground</h1>
    <div class="header-status">
      <div class="status-item">
        <span class="status-dot" id="camera-status"></span>
        <span>Camera</span>
      </div>
      <div class="status-item">
        <span class="status-dot" id="model-status"></span>
        <span id="model-status-text">No model</span>
      </div>
    </div>
  </header>

  <!-- Main Container -->
  <div class="main-container">
    <!-- Sidebar: Model Selection -->
    <aside class="sidebar">
      <div class="sidebar-section">
        <h3>Models</h3>
        <div class="model-list" id="model-list">
          <!-- Models will be populated by JS -->
        </div>
      </div>

      <div class="sidebar-section" id="model-settings">
        <h3>Settings</h3>
        <div id="settings-content">
          <p style="font-size: 12px; color: #666;">Select a model to see settings</p>
        </div>
      </div>

      <div class="sidebar-section">
        <h3>Visualization</h3>
        <div class="viz-options" id="viz-options">
          <!-- Visualization options populated by JS -->
        </div>
      </div>
    </aside>

    <!-- Video Area -->
    <main class="video-area">
      <div class="video-container" id="video-container">
        <video id="webcam" autoplay playsinline muted></video>
        <canvas id="output-canvas"></canvas>
        <div class="video-overlay">
          <div class="overlay-stat fps" id="fps-display">-- FPS</div>
          <div class="overlay-stat inference" id="inference-display">-- ms</div>
        </div>
        <div class="video-placeholder" id="video-placeholder">
          <div class="video-placeholder-icon">üì∑</div>
          <p>Click "Start Camera" to begin</p>
        </div>
      </div>

      <div class="controls-bar">
        <button class="control-btn primary" id="start-btn">
          <span>‚ñ∂</span> Start Camera
        </button>
        <button class="control-btn secondary" id="stop-btn" disabled>
          <span>‚èπ</span> Stop
        </button>
        <button class="control-btn secondary" id="screenshot-btn" disabled>
          <span>üì∏</span> Screenshot
        </button>
      </div>
    </main>

    <!-- Info Panel -->
    <aside class="info-panel">
      <div class="info-section">
        <h3>Performance</h3>
        <div class="metrics-grid">
          <div class="metric-card">
            <div class="metric-value" id="metric-fps">--</div>
            <div class="metric-label">FPS</div>
          </div>
          <div class="metric-card">
            <div class="metric-value" id="metric-inference">--</div>
            <div class="metric-label">Inference (ms)</div>
          </div>
          <div class="metric-card">
            <div class="metric-value" id="metric-resolution">--</div>
            <div class="metric-label">Resolution</div>
          </div>
          <div class="metric-card">
            <div class="metric-value" id="metric-model-size">--</div>
            <div class="metric-label">Model Size</div>
          </div>
        </div>
      </div>

      <div class="info-section">
        <h3 id="info-title">About</h3>
        <div class="info-text" id="info-content">
          <p>Select a model from the sidebar to see details about what it does and how it's used in the extension.</p>
        </div>
      </div>

      <div class="info-section">
        <h3>Usage in Extension</h3>
        <div class="info-text" id="usage-content">
          <p>Information about how the selected model is used (or could be used) in the Meet Camera Overlay extension.</p>
        </div>
      </div>
    </aside>
  </div>

  <!-- Scripts -->
  <script type="module">
    // ============================================
    // Model Registry
    // ============================================
    const MODELS = {
      'segmentation': {
        id: 'segmentation',
        name: 'Image Segmenter',
        icon: 'üë§',
        status: 'current',
        description: 'Separates people from background using MediaPipe Selfie Segmentation.',
        usage: 'Used to render wall art BEHIND the person. The segmentation mask determines which pixels belong to the person vs background.',
        modelSize: '~256 KB',
        settings: [
          { id: 'model-type', type: 'select', label: 'Model Type', options: ['Landscape', 'General'], default: 'Landscape' },
          { id: 'threshold', type: 'range', label: 'Confidence Threshold', min: 0, max: 1, step: 0.05, default: 0.5 }
        ],
        vizOptions: ['Mask Only', 'Colored Overlay', 'Background Blur', 'Background Replace']
      },
      'wall-detection': {
        id: 'wall-detection',
        name: 'Wall Detection',
        icon: 'üß±',
        status: 'current',
        description: 'Smart wall detection combining depth, color uniformity, vertical position, person mask, and ceiling detection.',
        usage: 'Helps users identify suitable wall areas for placing art. Uses depth data (when available) to find flat surfaces at consistent distances.',
        modelSize: 'N/A + Depth model',
        settings: [
          { id: 'grid-size', type: 'range', label: 'Grid Size', min: 4, max: 16, step: 1, default: 8 },
          { id: 'uniformity', type: 'range', label: 'Uniformity Threshold', min: 0, max: 50, step: 5, default: 25 },
          { id: 'vertical-weight', type: 'range', label: 'Vertical Position Weight', min: 0, max: 1, step: 0.1, default: 0.4 },
          { id: 'person-weight', type: 'range', label: 'Behind-Person Weight', min: 0, max: 1, step: 0.1, default: 0.2 },
          { id: 'depth-weight', type: 'range', label: 'Depth Weight', min: 0, max: 1, step: 0.1, default: 0.4 },
          { id: 'depth-update-interval', type: 'range', label: 'Depth Update (frames)', min: 15, max: 120, step: 15, default: 45 },
          { id: 'ceiling-detection', type: 'toggle', label: 'Ceiling Detection', default: true },
          { id: 'ceiling-height', type: 'range', label: 'Ceiling Height (%)', min: 10, max: 50, step: 5, default: 25 },
          { id: 'update-interval', type: 'range', label: 'Fast Update (frames)', min: 1, max: 15, step: 1, default: 3 }
        ],
        vizOptions: ['Smart Detection', 'Depth + Color', 'Factor Breakdown', 'Depth Flatness', 'Raw Factors']
      },
      'edge-detection': {
        id: 'edge-detection',
        name: 'Edge Detection',
        icon: 'üìê',
        status: 'current',
        description: 'Detects edges and lines in the video frame for snapping wall art boundaries.',
        usage: 'Allows wall art region corners to snap to detected edges like wall corners, shelves, and door frames.',
        modelSize: 'N/A (CPU)',
        settings: [
          { id: 'threshold', type: 'range', label: 'Detection Threshold', min: 10, max: 100, step: 5, default: 40 },
          { id: 'blur-radius', type: 'range', label: 'Blur Radius', min: 0, max: 5, step: 1, default: 1 },
          { id: 'min-length', type: 'range', label: 'Min Line Length', min: 5, max: 50, step: 5, default: 15 }
        ],
        vizOptions: ['All Edges', 'Strong Edges Only', 'Edge Angles']
      },
      'depth-estimation': {
        id: 'depth-estimation',
        name: 'Depth Estimation',
        icon: 'üåä',
        status: 'current',
        description: 'Real depth estimation using Depth Anything V2 Small model via Transformers.js. Estimates per-pixel depth.',
        usage: 'Identifies wall planes by finding regions with consistent depth. Enables accurate wall detection by distinguishing surfaces at different distances.',
        modelSize: '27 MB (INT8)',
        settings: [
          { id: 'device', type: 'select', label: 'Device', options: ['WebGPU (Fast)', 'WASM (Compatible)'], default: 'WebGPU (Fast)' },
          { id: 'update-interval', type: 'range', label: 'Update Every N Frames', min: 1, max: 60, step: 1, default: 15 },
          { id: 'resolution-scale', type: 'range', label: 'Resolution Scale', min: 0.25, max: 1, step: 0.25, default: 0.5 }
        ],
        vizOptions: ['Depth Map', 'Depth Contours', 'Near/Far Regions', 'Wall Planes']
      },
      'stabilization': {
        id: 'stabilization',
        name: 'Stabilization',
        icon: 'üéØ',
        status: 'comparison',
        description: 'Compare different approaches to keeping wall art stable when the camera or person moves.',
        usage: 'Current: Simple smoothing (jiggle compensator). Future: Feature tracking could lock art to actual wall position.',
        modelSize: 'N/A',
        settings: [
          { id: 'mode', type: 'select', label: 'Stabilization Mode', options: ['None', 'Current (Smoothing)', 'Feature Tracking (Demo)'], default: 'Current (Smoothing)' },
          { id: 'smoothing', type: 'range', label: 'Smoothing Factor', min: 0, max: 1, step: 0.1, default: 0.7 }
        ],
        vizOptions: ['Position Trail', 'Jitter Graph', 'Side-by-Side']
      },
      'combined': {
        id: 'combined',
        name: 'Combined Pipeline',
        icon: 'üîó',
        status: 'demo',
        description: 'See all models working together in the full wall art rendering pipeline.',
        usage: 'Demonstrates how segmentation, wall detection, and edge detection work together for wall art placement.',
        modelSize: 'Multiple',
        settings: [
          { id: 'show-segmentation', type: 'toggle', label: 'Show Segmentation', default: true },
          { id: 'show-walls', type: 'toggle', label: 'Show Wall Detection', default: true },
          { id: 'show-edges', type: 'toggle', label: 'Show Edges', default: false }
        ],
        vizOptions: ['Full Pipeline', 'Layers View']
      }
    };

    // ============================================
    // App State
    // ============================================
    const state = {
      currentModel: null,
      isRunning: false,
      stream: null,
      animationId: null,
      fps: 0,
      inferenceTime: 0,
      frameCount: 0,
      lastFpsUpdate: 0,
      settings: {},
      vizMode: 0,
      models: {
        segmenter: null,
        depthEstimator: null
      }
    };

    // ============================================
    // DOM Elements
    // ============================================
    const elements = {
      loadingOverlay: document.getElementById('loading-overlay'),
      errorToast: document.getElementById('error-toast'),
      cameraStatus: document.getElementById('camera-status'),
      modelStatus: document.getElementById('model-status'),
      modelStatusText: document.getElementById('model-status-text'),
      modelList: document.getElementById('model-list'),
      settingsContent: document.getElementById('settings-content'),
      vizOptions: document.getElementById('viz-options'),
      videoContainer: document.getElementById('video-container'),
      webcam: document.getElementById('webcam'),
      canvas: document.getElementById('output-canvas'),
      videoPlaceholder: document.getElementById('video-placeholder'),
      fpsDisplay: document.getElementById('fps-display'),
      inferenceDisplay: document.getElementById('inference-display'),
      startBtn: document.getElementById('start-btn'),
      stopBtn: document.getElementById('stop-btn'),
      screenshotBtn: document.getElementById('screenshot-btn'),
      metricFps: document.getElementById('metric-fps'),
      metricInference: document.getElementById('metric-inference'),
      metricResolution: document.getElementById('metric-resolution'),
      metricModelSize: document.getElementById('metric-model-size'),
      infoTitle: document.getElementById('info-title'),
      infoContent: document.getElementById('info-content'),
      usageContent: document.getElementById('usage-content')
    };

    const ctx = elements.canvas.getContext('2d');

    // ============================================
    // Initialize
    // ============================================
    function init() {
      renderModelList();
      setupEventListeners();
      hideLoading();
      selectModel('segmentation');
    }

    function renderModelList() {
      elements.modelList.innerHTML = Object.values(MODELS).map(model => `
        <div class="model-item" data-model="${model.id}">
          <div class="model-icon">${model.icon}</div>
          <div class="model-info">
            <div class="model-name">${model.name}</div>
            <div class="model-status">${getStatusText(model.status)}</div>
          </div>
          <span class="model-badge ${model.status}">${model.status}</span>
        </div>
      `).join('');
    }

    function getStatusText(status) {
      switch (status) {
        case 'current': return 'In use';
        case 'potential': return 'Could add';
        case 'comparison': return 'Compare';
        case 'demo': return 'Demo';
        default: return status;
      }
    }

    function setupEventListeners() {
      // Model selection
      elements.modelList.addEventListener('click', (e) => {
        const modelItem = e.target.closest('.model-item');
        if (modelItem) {
          selectModel(modelItem.dataset.model);
        }
      });

      // Camera controls
      elements.startBtn.addEventListener('click', startCamera);
      elements.stopBtn.addEventListener('click', stopCamera);
      elements.screenshotBtn.addEventListener('click', takeScreenshot);

      // Visualization options
      elements.vizOptions.addEventListener('click', (e) => {
        if (e.target.classList.contains('viz-option')) {
          const index = parseInt(e.target.dataset.index);
          setVizMode(index);
        }
      });
    }

    // ============================================
    // Model Selection
    // ============================================
    function selectModel(modelId) {
      const model = MODELS[modelId];
      if (!model) return;

      state.currentModel = model;
      state.vizMode = 0; // Reset viz mode when switching models

      // Update UI
      document.querySelectorAll('.model-item').forEach(item => {
        item.classList.toggle('active', item.dataset.model === modelId);
      });

      // Update info panel
      elements.infoTitle.textContent = model.name;
      elements.infoContent.innerHTML = `<p>${model.description}</p>`;
      elements.usageContent.innerHTML = `<p>${model.usage}</p>`;
      elements.metricModelSize.textContent = model.modelSize;

      // Render settings
      renderSettings(model);

      // Render viz options
      renderVizOptions(model);

      // Update model status indicator
      updateModelStatusDisplay();

      // If camera is running, load the model for this view
      if (state.isRunning) {
        loadCurrentModel();
      }
    }

    function updateModelStatusDisplay() {
      const modelId = state.currentModel?.id;
      let statusText = state.currentModel?.name || 'None';
      let isLoaded = false;

      // Check if the required model for this view is loaded
      switch (modelId) {
        case 'segmentation':
          isLoaded = !!state.models.segmenter;
          statusText += isLoaded ? ' ‚úì' : ' (not loaded)';
          break;
        case 'depth-estimation':
          isLoaded = !!state.models.depthEstimator;
          statusText += isLoaded ? ' ‚úì' : ' (not loaded)';
          break;
        case 'wall-detection':
        case 'edge-detection':
        case 'stabilization':
          isLoaded = true; // CPU-based, always available
          statusText += ' ‚úì';
          break;
        case 'combined':
          const segLoaded = !!state.models.segmenter;
          statusText += segLoaded ? ' (seg ‚úì)' : ' (seg pending)';
          isLoaded = segLoaded;
          break;
        default:
          isLoaded = true;
      }

      elements.modelStatusText.textContent = statusText;
      elements.modelStatus.classList.remove('active', 'loading', 'error');
      elements.modelStatus.classList.add(isLoaded ? 'active' : 'loading');
    }

    function renderSettings(model) {
      if (!model.settings || model.settings.length === 0) {
        elements.settingsContent.innerHTML = '<p style="font-size: 12px; color: #666;">No settings for this model</p>';
        return;
      }

      elements.settingsContent.innerHTML = model.settings.map(setting => {
        if (setting.type === 'select') {
          return `
            <div class="settings-group">
              <label>${setting.label}</label>
              <select id="setting-${setting.id}" data-setting="${setting.id}">
                ${setting.options.map(opt => `<option value="${opt}" ${opt === setting.default ? 'selected' : ''}>${opt}</option>`).join('')}
              </select>
            </div>
          `;
        } else if (setting.type === 'range') {
          const value = state.settings[setting.id] ?? setting.default;
          return `
            <div class="settings-group">
              <label>${setting.label}</label>
              <input type="range" id="setting-${setting.id}" data-setting="${setting.id}"
                min="${setting.min}" max="${setting.max}" step="${setting.step}" value="${value}">
              <div class="range-value" id="value-${setting.id}">${value}</div>
            </div>
          `;
        } else if (setting.type === 'toggle') {
          const checked = state.settings[setting.id] ?? setting.default;
          return `
            <div class="toggle-row">
              <span class="toggle-label">${setting.label}</span>
              <label class="toggle-switch">
                <input type="checkbox" id="setting-${setting.id}" data-setting="${setting.id}" ${checked ? 'checked' : ''}>
                <span class="toggle-slider"></span>
              </label>
            </div>
          `;
        }
        return '';
      }).join('');

      // Add event listeners for settings
      elements.settingsContent.querySelectorAll('input, select').forEach(input => {
        input.addEventListener('input', (e) => {
          const settingId = e.target.dataset.setting;
          let value = e.target.type === 'checkbox' ? e.target.checked : e.target.value;
          if (e.target.type === 'range') {
            value = parseFloat(value);
            const valueDisplay = document.getElementById(`value-${settingId}`);
            if (valueDisplay) valueDisplay.textContent = value;
          }
          state.settings[settingId] = value;
        });
      });
    }

    function renderVizOptions(model) {
      if (!model.vizOptions || model.vizOptions.length === 0) {
        elements.vizOptions.innerHTML = '<p style="font-size: 12px; color: #666;">No options</p>';
        return;
      }

      elements.vizOptions.innerHTML = model.vizOptions.map((opt, i) => `
        <div class="viz-option ${i === state.vizMode ? 'active' : ''}" data-index="${i}">${opt}</div>
      `).join('');
    }

    function setVizMode(index) {
      state.vizMode = index;
      document.querySelectorAll('.viz-option').forEach((opt, i) => {
        opt.classList.toggle('active', i === index);
      });
    }

    // ============================================
    // Camera
    // ============================================
    async function startCamera() {
      try {
        showLoading('Starting camera...');
        
        state.stream = await navigator.mediaDevices.getUserMedia({
          video: { width: { ideal: 1280 }, height: { ideal: 720 }, facingMode: 'user' }
        });

        elements.webcam.srcObject = state.stream;
        await elements.webcam.play();

        // Set canvas size
        elements.canvas.width = elements.webcam.videoWidth;
        elements.canvas.height = elements.webcam.videoHeight;

        // Update UI
        elements.videoPlaceholder.style.display = 'none';
        elements.cameraStatus.classList.add('active');
        elements.startBtn.disabled = true;
        elements.stopBtn.disabled = false;
        elements.screenshotBtn.disabled = false;

        // Update resolution metric
        elements.metricResolution.textContent = `${elements.webcam.videoWidth}x${elements.webcam.videoHeight}`;

        state.isRunning = true;
        hideLoading();

        // Load model if needed
        await loadCurrentModel();

        // Start render loop
        requestAnimationFrame(renderLoop);

      } catch (error) {
        console.error('Camera error:', error);
        showError('Failed to access camera: ' + error.message);
        hideLoading();
      }
    }

    function stopCamera() {
      state.isRunning = false;

      if (state.animationId) {
        cancelAnimationFrame(state.animationId);
      }

      if (state.stream) {
        state.stream.getTracks().forEach(track => track.stop());
        state.stream = null;
      }

      elements.webcam.srcObject = null;
      elements.videoPlaceholder.style.display = 'flex';
      elements.cameraStatus.classList.remove('active');
      elements.startBtn.disabled = false;
      elements.stopBtn.disabled = true;
      elements.screenshotBtn.disabled = true;

      // Clear canvas
      ctx.clearRect(0, 0, elements.canvas.width, elements.canvas.height);
    }

    // ============================================
    // Model Loading
    // ============================================
    async function loadCurrentModel() {
      if (!state.currentModel) return;

      const modelId = state.currentModel.id;
      console.log(`[Model] Loading model for: ${modelId}`);

      elements.modelStatus.classList.remove('active', 'error');
      elements.modelStatus.classList.add('loading');

      try {
        switch (modelId) {
          case 'segmentation':
            await loadSegmentationModel();
            break;
          case 'depth-estimation':
            await loadDepthModel();
            break;
          case 'wall-detection':
            // Wall detection benefits from both segmentation and depth
            console.log('[Model] Wall detection: loading segmentation...');
            await loadSegmentationModel();
            // Start loading depth in background (don't await - it's slow)
            console.log('[Model] Wall detection: starting depth load in background...');
            loadDepthModel().catch(e => console.warn('[Model] Depth load failed:', e));
            break;
          case 'combined':
            // Combined needs segmentation
            await loadSegmentationModel();
            break;
          // Other models don't need async loading
          default:
            console.log(`[Model] ${modelId} is CPU-based, no loading needed`);
        }

        console.log(`[Model] ${modelId} ready`);
        updateModelStatusDisplay();
      } catch (error) {
        console.error('[Model] Load error:', error);
        elements.modelStatus.classList.remove('loading');
        elements.modelStatus.classList.add('error');
        showError('Failed to load model: ' + error.message);
        updateModelStatusDisplay();
      }
    }

    async function loadSegmentationModel() {
      if (state.models.segmenter) return;

      showLoading('Loading MediaPipe Image Segmenter...');

      // Dynamic import MediaPipe Tasks Vision
      const vision = await import('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/+esm');
      
      const { ImageSegmenter, FilesetResolver } = vision;

      const wasmFileset = await FilesetResolver.forVisionTasks(
        'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm'
      );

      state.models.segmenter = await ImageSegmenter.createFromOptions(wasmFileset, {
        baseOptions: {
          modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_segmenter_landscape/float16/latest/selfie_segmenter_landscape.tflite',
          delegate: 'GPU'
        },
        runningMode: 'VIDEO',
        outputCategoryMask: true,
        outputConfidenceMasks: false
      });

      hideLoading();
    }

    async function loadDepthModel() {
      if (state.models.depthEstimator) {
        console.log('[Depth] Model already loaded');
        return;
      }

      const deviceSetting = state.settings['device'] || 'WebGPU (Fast)';
      const device = deviceSetting.includes('WebGPU') ? 'webgpu' : 'wasm';

      console.log(`[Depth] Starting to load Depth Anything V2 with device: ${device}`);
      showLoading(`Loading Depth Anything V2 (${device.toUpperCase()})... This may take a moment on first load.`);

      try {
        // Dynamic import Transformers.js
        console.log('[Depth] Importing Transformers.js...');
        const { pipeline, env } = await import('https://cdn.jsdelivr.net/npm/@huggingface/transformers@3');
        console.log('[Depth] Transformers.js imported successfully');

        // Configure for browser use
        env.allowLocalModels = false;
        env.useBrowserCache = true;

        // Check WebGPU availability
        let actualDevice = device;
        if (device === 'webgpu') {
          if (!navigator.gpu) {
            console.warn('[Depth] WebGPU not available, falling back to WASM');
            actualDevice = 'wasm';
            showLoading('WebGPU not available. Loading with WASM (slower)...');
          } else {
            console.log('[Depth] WebGPU is available');
          }
        }

        // Load the depth estimation pipeline
        console.log(`[Depth] Loading pipeline on ${actualDevice}... (this downloads ~27MB on first run)`);
        state.models.depthEstimator = await pipeline(
          'depth-estimation',
          'onnx-community/depth-anything-v2-small',
          {
            device: actualDevice,
            dtype: actualDevice === 'webgpu' ? 'fp32' : 'q8'
          }
        );

        console.log('[Depth] ‚úì Model loaded successfully on', actualDevice);
        hideLoading();
      } catch (error) {
        console.error('[Depth] Load error:', error);
        hideLoading();
        throw new Error(`Failed to load depth model: ${error.message}`);
      }
    }

    // ============================================
    // Render Loop
    // ============================================
    let lastFrameTime = 0;

    function renderLoop(timestamp) {
      if (!state.isRunning) return;

      state.animationId = requestAnimationFrame(renderLoop);

      // Calculate FPS
      state.frameCount++;
      if (timestamp - state.lastFpsUpdate >= 1000) {
        state.fps = state.frameCount;
        state.frameCount = 0;
        state.lastFpsUpdate = timestamp;
        updateMetrics();
      }

      // Run inference
      const startTime = performance.now();
      processFrame();
      state.inferenceTime = performance.now() - startTime;
    }

    function processFrame() {
      if (!state.currentModel) {
        // Just draw video
        ctx.drawImage(elements.webcam, 0, 0);
        return;
      }

      switch (state.currentModel.id) {
        case 'segmentation':
          processSegmentation();
          break;
        case 'wall-detection':
          processWallDetection();
          break;
        case 'edge-detection':
          processEdgeDetection();
          break;
        case 'depth-estimation':
          processDepthEstimation();
          break;
        case 'stabilization':
          processStabilization();
          break;
        case 'combined':
          processCombined();
          break;
        default:
          ctx.drawImage(elements.webcam, 0, 0);
      }
    }

    // ============================================
    // Model Processing Functions
    // ============================================
    function processSegmentation() {
      const segmenter = state.models.segmenter;
      
      if (!segmenter) {
        ctx.drawImage(elements.webcam, 0, 0);
        return;
      }

      // Run segmentation
      const result = segmenter.segmentForVideo(elements.webcam, performance.now());
      
      // Draw base video
      ctx.drawImage(elements.webcam, 0, 0);

      if (result.categoryMask) {
        const mask = result.categoryMask;
        const maskData = mask.getAsUint8Array();
        const width = mask.width;
        const height = mask.height;

        // Create ImageData for mask visualization
        const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
        const pixels = imageData.data;

        // Apply visualization based on mode
        const vizMode = state.currentModel.vizOptions[state.vizMode];

        for (let i = 0; i < maskData.length; i++) {
          const isPerson = maskData[i] > 0;
          const pixelIndex = i * 4;

          if (vizMode === 'Mask Only') {
            pixels[pixelIndex] = isPerson ? 255 : 0;
            pixels[pixelIndex + 1] = isPerson ? 255 : 0;
            pixels[pixelIndex + 2] = isPerson ? 255 : 0;
          } else if (vizMode === 'Colored Overlay') {
            if (isPerson) {
              pixels[pixelIndex] = Math.min(255, pixels[pixelIndex] + 50);
              pixels[pixelIndex + 2] = Math.max(0, pixels[pixelIndex + 2] - 50);
            }
          } else if (vizMode === 'Background Blur') {
            // Simplified blur effect - just darken background
            if (!isPerson) {
              pixels[pixelIndex] = pixels[pixelIndex] * 0.3;
              pixels[pixelIndex + 1] = pixels[pixelIndex + 1] * 0.3;
              pixels[pixelIndex + 2] = pixels[pixelIndex + 2] * 0.3;
            }
          } else if (vizMode === 'Background Replace') {
            if (!isPerson) {
              // Replace with solid color
              pixels[pixelIndex] = 30;
              pixels[pixelIndex + 1] = 30;
              pixels[pixelIndex + 2] = 60;
            }
          }
        }

        ctx.putImageData(imageData, 0, 0);
        mask.close();
      }
    }

    // Cache for wall detection results
    const wallDetectionCache = {
      frameCount: 0,
      depthFrameCount: 0,
      cellScores: null,
      ceilingBoundaryY: 0,
      personBounds: null,
      // Depth-derived data (updated less frequently)
      depthScores: null,      // Per-cell depth scores
      depthFlatness: null,    // Per-cell flatness (low variance = flat)
      depthDistance: null,    // Per-cell normalized distance
      isUpdatingDepth: false
    };

    function processWallDetection() {
      ctx.drawImage(elements.webcam, 0, 0);

      const gridSize = state.settings['grid-size'] || 8;
      const updateInterval = state.settings['update-interval'] ?? 3;
      const depthUpdateInterval = state.settings['depth-update-interval'] ?? 45;
      const vizMode = state.currentModel.vizOptions[state.vizMode];

      const cellWidth = Math.floor(elements.canvas.width / gridSize);
      const cellHeight = Math.floor(elements.canvas.height / gridSize);

      // Check if we should update depth analysis (slow, infrequent)
      wallDetectionCache.depthFrameCount++;
      const depthWeight = state.settings['depth-weight'] ?? 0.4;
      if (depthWeight > 0 &&
          wallDetectionCache.depthFrameCount >= depthUpdateInterval &&
          !wallDetectionCache.isUpdatingDepth) {
        wallDetectionCache.depthFrameCount = 0;
        updateDepthAnalysis(gridSize);
      }

      // Check if we should update fast analysis
      wallDetectionCache.frameCount++;
      const shouldUpdate = wallDetectionCache.frameCount >= updateInterval || !wallDetectionCache.cellScores;

      if (shouldUpdate) {
        wallDetectionCache.frameCount = 0;
        updateWallDetectionAnalysis(gridSize, cellWidth, cellHeight);
      }

      // Always render using cached results
      if (wallDetectionCache.cellScores) {
        renderSmartWallViz(
          wallDetectionCache.cellScores,
          vizMode,
          cellWidth,
          cellHeight
        );
      }
    }

    // Update depth-based analysis (runs infrequently)
    async function updateDepthAnalysis(gridSize) {
      const depthEstimator = state.models.depthEstimator;
      if (!depthEstimator || wallDetectionCache.isUpdatingDepth) return;

      wallDetectionCache.isUpdatingDepth = true;
      console.log('[WallDetect] Updating depth analysis...');

      try {
        // Get depth map
        const tempCanvas = document.createElement('canvas');
        const scale = 0.5;
        tempCanvas.width = Math.round(elements.canvas.width * scale);
        tempCanvas.height = Math.round(elements.canvas.height * scale);
        const tempCtx = tempCanvas.getContext('2d');
        tempCtx.drawImage(elements.webcam, 0, 0, tempCanvas.width, tempCanvas.height);

        const dataUrl = tempCanvas.toDataURL('image/jpeg', 0.8);
        const result = await depthEstimator(dataUrl);

        if (result && result.predicted_depth) {
          const depthTensor = result.predicted_depth;
          const depthData = depthTensor.data;
          const depthWidth = depthTensor.dims[1];
          const depthHeight = depthTensor.dims[0];

          // Find depth range
          let minDepth = Infinity, maxDepth = -Infinity;
          for (let i = 0; i < depthData.length; i++) {
            minDepth = Math.min(minDepth, depthData[i]);
            maxDepth = Math.max(maxDepth, depthData[i]);
          }
          const depthRange = maxDepth - minDepth || 1;

          // Calculate per-cell depth scores
          const cellWidth = Math.floor(depthWidth / gridSize);
          const cellHeight = Math.floor(depthHeight / gridSize);

          const depthScores = [];
          const depthFlatness = [];
          const depthDistance = [];

          for (let gy = 0; gy < gridSize; gy++) {
            const rowScores = [];
            const rowFlatness = [];
            const rowDistance = [];

            for (let gx = 0; gx < gridSize; gx++) {
              const startX = gx * cellWidth;
              const startY = gy * cellHeight;

              // Sample depth values in this cell
              let sumDepth = 0, count = 0;
              const samples = [];
              for (let dy = 0; dy < cellHeight; dy += 2) {
                for (let dx = 0; dx < cellWidth; dx += 2) {
                  const idx = (startY + dy) * depthWidth + (startX + dx);
                  if (idx < depthData.length) {
                    const d = depthData[idx];
                    samples.push(d);
                    sumDepth += d;
                    count++;
                  }
                }
              }

              const avgDepth = count > 0 ? sumDepth / count : 0;
              const normalizedDepth = (avgDepth - minDepth) / depthRange;

              // Calculate variance (flatness indicator)
              let variance = 0;
              for (const s of samples) {
                variance += (s - avgDepth) ** 2;
              }
              variance = count > 0 ? Math.sqrt(variance / count) / depthRange : 1;

              // Flatness score: low variance = flat surface = good for wall
              const flatnessScore = Math.max(0, 1 - variance * 10);

              // Distance score: further = more likely wall (not person)
              // Walls are typically in the 0.5-0.9 normalized depth range
              const distanceScore = normalizedDepth > 0.3 ?
                Math.min(1, (normalizedDepth - 0.3) / 0.5) : 0;

              // Combined depth score
              const depthScore = flatnessScore * 0.6 + distanceScore * 0.4;

              rowScores.push(depthScore);
              rowFlatness.push(flatnessScore);
              rowDistance.push(normalizedDepth);
            }

            depthScores.push(rowScores);
            depthFlatness.push(rowFlatness);
            depthDistance.push(rowDistance);
          }

          wallDetectionCache.depthScores = depthScores;
          wallDetectionCache.depthFlatness = depthFlatness;
          wallDetectionCache.depthDistance = depthDistance;

          console.log('[WallDetect] Depth analysis complete');
        }
      } catch (error) {
        console.error('[WallDetect] Depth analysis error:', error);
      } finally {
        wallDetectionCache.isUpdatingDepth = false;
      }
    }

    function updateWallDetectionAnalysis(gridSize, cellWidth, cellHeight) {
      const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
      const uniformityThreshold = state.settings['uniformity'] || 25;
      const verticalWeight = state.settings['vertical-weight'] ?? 0.4;
      const personWeight = state.settings['person-weight'] ?? 0.2;
      const depthWeight = state.settings['depth-weight'] ?? 0.4;
      const ceilingDetectionEnabled = state.settings['ceiling-detection'] ?? true;
      const ceilingHeightPercent = state.settings['ceiling-height'] ?? 25;

      // Get person segmentation mask if available
      let personMask = null;
      if (state.models.segmenter && personWeight > 0) {
        const result = state.models.segmenter.segmentForVideo(elements.webcam, performance.now());
        if (result.categoryMask) {
          personMask = result.categoryMask.getAsUint8Array();
          wallDetectionCache.personBounds = findPersonBounds(personMask, elements.canvas.width);
          result.categoryMask.close();
        }
      }

      // Detect ceiling boundary
      if (ceilingDetectionEnabled) {
        wallDetectionCache.ceilingBoundaryY = detectCeilingBoundary(imageData, ceilingHeightPercent);
      } else {
        wallDetectionCache.ceilingBoundaryY = 0;
      }

      const ceilingBoundaryY = wallDetectionCache.ceilingBoundaryY;
      const cellScores = [];
      const hasDepthData = wallDetectionCache.depthScores !== null;

      // Normalize weights
      let totalWeight = verticalWeight + personWeight;
      if (hasDepthData) totalWeight += depthWeight;
      const uniformityWeight = Math.max(0.1, 1 - totalWeight);

      // Analyze each grid cell
      for (let gy = 0; gy < gridSize; gy++) {
        const row = [];
        for (let gx = 0; gx < gridSize; gx++) {
          const x = gx * cellWidth;
          const y = gy * cellHeight;
          const centerY = y + cellHeight / 2;

          // 1. Color uniformity score
          const { avgColor, variance } = analyzeCellColor(imageData, x, y, cellWidth, cellHeight);
          const uniformityScore = Math.max(0, 1 - variance / (uniformityThreshold * 2));

          // 2. Vertical position score
          const normalizedY = gy / gridSize;
          const verticalScore = calculateVerticalScore(normalizedY, ceilingBoundaryY / elements.canvas.height);

          // 3. Person proximity score
          const personScore = personMask
            ? calculatePersonProximityScoreFast(personMask, x, y, cellWidth, cellHeight, wallDetectionCache.personBounds)
            : 0.5;

          // 4. Depth score (from cached depth analysis)
          let depthScore = 0.5; // Neutral if no depth data
          let depthFlatness = 0;
          let depthDistance = 0;
          if (hasDepthData && wallDetectionCache.depthScores[gy]) {
            depthScore = wallDetectionCache.depthScores[gy][gx] ?? 0.5;
            depthFlatness = wallDetectionCache.depthFlatness[gy]?.[gx] ?? 0;
            depthDistance = wallDetectionCache.depthDistance[gy]?.[gx] ?? 0;
          }

          // 5. Ceiling penalty
          const ceilingPenalty = (ceilingDetectionEnabled && centerY < ceilingBoundaryY) ? 0.2 : 1.0;

          // Calculate combined wall score
          let wallScore;
          if (hasDepthData) {
            wallScore = (
              uniformityScore * uniformityWeight +
              verticalScore * verticalWeight +
              personScore * personWeight +
              depthScore * depthWeight
            ) * ceilingPenalty;
          } else {
            // Fallback without depth
            wallScore = (
              uniformityScore * (uniformityWeight + depthWeight) +
              verticalScore * verticalWeight +
              personScore * personWeight
            ) * ceilingPenalty;
          }
          wallScore = Math.max(0, Math.min(1, wallScore));

          row.push({
            x, y,
            uniformityScore,
            verticalScore,
            personScore,
            depthScore,
            depthFlatness,
            depthDistance,
            ceilingPenalty,
            wallScore,
            avgColor,
            variance,
            hasDepthData,
            isWall: wallScore > 0.5
          });
        }
        cellScores.push(row);
      }

      wallDetectionCache.cellScores = cellScores;
    }

    // Pre-compute person bounds once per frame instead of per-cell
    function findPersonBounds(personMask, canvasWidth) {
      let minX = canvasWidth;
      let maxX = 0;
      let found = false;

      // Sample every 8th pixel for speed
      for (let i = 0; i < personMask.length; i += 8) {
        if (personMask[i] > 0) {
          const px = i % canvasWidth;
          minX = Math.min(minX, px);
          maxX = Math.max(maxX, px);
          found = true;
        }
      }

      return found ? { minX, maxX, centerX: (minX + maxX) / 2, width: maxX - minX } : null;
    }

    // Fast person proximity using pre-computed bounds
    function calculatePersonProximityScoreFast(personMask, cellX, cellY, cellWidth, cellHeight, personBounds) {
      if (!personBounds) return 0.5;

      const cellCenterX = cellX + cellWidth / 2;

      // Quick check if any person pixels in this cell (sample sparsely)
      let personPixelsInCell = 0;
      let totalPixelsChecked = 0;
      for (let py = cellY; py < cellY + cellHeight; py += 8) {
        for (let px = cellX; px < cellX + cellWidth; px += 8) {
          const idx = py * elements.canvas.width + px;
          if (idx < personMask.length) {
            totalPixelsChecked++;
            if (personMask[idx] > 0) personPixelsInCell++;
          }
        }
      }

      const personRatio = totalPixelsChecked > 0 ? personPixelsInCell / totalPixelsChecked : 0;
      if (personRatio > 0.1) return 0; // Cell contains person

      // Score based on distance from person
      const distanceFromPerson = Math.abs(cellCenterX - personBounds.centerX);

      if (distanceFromPerson < personBounds.width * 0.5) {
        return 0.9; // Directly behind person
      } else if (distanceFromPerson < personBounds.width) {
        return 0.7; // Near person
      } else {
        return 0.4; // Far from person
      }
    }

    // Calculate vertical position score - bell curve favoring middle of frame
    function calculateVerticalScore(normalizedY, ceilingY) {
      // If above ceiling boundary, strongly penalize
      if (normalizedY < ceilingY) {
        return 0.1;
      }

      // Bell curve centered at 0.5 (middle of frame)
      // Walls are typically in the middle vertical third
      const center = 0.5;
      const sigma = 0.3;
      const score = Math.exp(-Math.pow(normalizedY - center, 2) / (2 * sigma * sigma));
      return score;
    }

    // Detect ceiling boundary using horizontal edge detection
    function detectCeilingBoundary(imageData, maxHeightPercent) {
      const width = imageData.width;
      const height = imageData.height;
      const data = imageData.data;
      const maxY = Math.floor(height * maxHeightPercent / 100);

      // Look for strong horizontal edges in the top portion
      let strongestEdgeY = 0;
      let strongestEdgeStrength = 0;

      // Convert to grayscale and detect horizontal edges
      for (let y = 5; y < maxY; y++) {
        let edgeStrength = 0;

        // Sample across the width
        for (let x = 10; x < width - 10; x += 5) {
          // Vertical gradient (detect horizontal lines)
          const above = ((y - 2) * width + x) * 4;
          const below = ((y + 2) * width + x) * 4;

          const grayAbove = (data[above] + data[above + 1] + data[above + 2]) / 3;
          const grayBelow = (data[below] + data[below + 1] + data[below + 2]) / 3;

          edgeStrength += Math.abs(grayAbove - grayBelow);
        }

        // Normalize by number of samples
        edgeStrength /= Math.floor((width - 20) / 5);

        if (edgeStrength > strongestEdgeStrength && edgeStrength > 15) {
          strongestEdgeStrength = edgeStrength;
          strongestEdgeY = y;
        }
      }

      return strongestEdgeY;
    }

    // Render smart wall detection visualization
    function renderSmartWallViz(cellScores, vizMode, cellWidth, cellHeight) {
      const hasDepthData = cellScores[0]?.[0]?.hasDepthData;
      const ceilingBoundaryY = wallDetectionCache.ceilingBoundaryY;

      for (let gy = 0; gy < cellScores.length; gy++) {
        for (let gx = 0; gx < cellScores[gy].length; gx++) {
          const cell = cellScores[gy][gx];
          const { x, y, wallScore, uniformityScore, verticalScore, personScore, depthScore, depthFlatness, depthDistance, isWall } = cell;

          if (vizMode === 'Smart Detection') {
            // Main view: highlight detected walls with confidence
            if (isWall) {
              ctx.fillStyle = `rgba(74, 222, 128, ${0.15 + wallScore * 0.45})`;
              ctx.fillRect(x, y, cellWidth, cellHeight);

              // Border for high confidence
              if (wallScore > 0.7) {
                ctx.strokeStyle = 'rgba(74, 222, 128, 0.8)';
                ctx.lineWidth = 2;
                ctx.strokeRect(x + 1, y + 1, cellWidth - 2, cellHeight - 2);
              }
            }
          } else if (vizMode === 'Depth + Color') {
            // Show depth influence vs color uniformity
            if (hasDepthData) {
              // Blue = depth score, Green = uniformity score
              const r = 0;
              const g = Math.floor(uniformityScore * 200);
              const b = Math.floor(depthScore * 200);
              ctx.fillStyle = `rgba(${r}, ${g}, ${b}, 0.5)`;
              ctx.fillRect(x, y, cellWidth, cellHeight);
            } else {
              // No depth - just show uniformity in green
              ctx.fillStyle = `rgba(0, ${Math.floor(uniformityScore * 200)}, 0, 0.4)`;
              ctx.fillRect(x, y, cellWidth, cellHeight);
            }
          } else if (vizMode === 'Factor Breakdown') {
            // Show all 5 factors as bars
            const numFactors = hasDepthData ? 5 : 4;
            const barHeight = cellHeight - 6;
            const barWidth = (cellWidth - 6) / numFactors;
            const barX = x + 3;
            const barY = y + 3;

            // Uniformity (blue)
            ctx.fillStyle = `rgba(59, 130, 246, ${uniformityScore})`;
            ctx.fillRect(barX, barY, barWidth - 1, barHeight);

            // Vertical (yellow)
            ctx.fillStyle = `rgba(250, 204, 21, ${verticalScore})`;
            ctx.fillRect(barX + barWidth, barY, barWidth - 1, barHeight);

            // Person (purple)
            ctx.fillStyle = `rgba(168, 85, 247, ${personScore})`;
            ctx.fillRect(barX + barWidth * 2, barY, barWidth - 1, barHeight);

            // Depth (cyan) - if available
            if (hasDepthData) {
              ctx.fillStyle = `rgba(34, 211, 238, ${depthScore})`;
              ctx.fillRect(barX + barWidth * 3, barY, barWidth - 1, barHeight);

              // Final score (green)
              ctx.fillStyle = `rgba(74, 222, 128, ${wallScore})`;
              ctx.fillRect(barX + barWidth * 4, barY, barWidth - 1, barHeight);
            } else {
              // Final score (green)
              ctx.fillStyle = `rgba(74, 222, 128, ${wallScore})`;
              ctx.fillRect(barX + barWidth * 3, barY, barWidth - 1, barHeight);
            }
          } else if (vizMode === 'Depth Flatness') {
            // Show depth flatness (how flat/planar the surface is)
            if (hasDepthData) {
              const hue = depthFlatness * 120; // Red = not flat, Green = flat
              ctx.fillStyle = `hsla(${hue}, 70%, 50%, 0.5)`;
              ctx.fillRect(x, y, cellWidth, cellHeight);

              // Show distance as text
              ctx.fillStyle = 'rgba(255, 255, 255, 0.9)';
              ctx.font = '9px monospace';
              ctx.textAlign = 'center';
              ctx.fillText((depthDistance * 100).toFixed(0), x + cellWidth / 2, y + cellHeight / 2 + 3);
            } else {
              ctx.fillStyle = 'rgba(100, 100, 100, 0.3)';
              ctx.fillRect(x, y, cellWidth, cellHeight);
            }
          } else if (vizMode === 'Raw Factors') {
            // Grid with all scores as text
            ctx.strokeStyle = 'rgba(255, 255, 255, 0.3)';
            ctx.lineWidth = 1;
            ctx.strokeRect(x, y, cellWidth, cellHeight);

            ctx.fillStyle = 'rgba(255, 255, 255, 0.9)';
            ctx.font = '8px monospace';
            ctx.textAlign = 'left';

            const lines = [
              `U:${(uniformityScore * 100).toFixed(0)}`,
              `V:${(verticalScore * 100).toFixed(0)}`,
              `P:${(personScore * 100).toFixed(0)}`,
              hasDepthData ? `D:${(depthScore * 100).toFixed(0)}` : 'D:--',
              `=${(wallScore * 100).toFixed(0)}`
            ];

            lines.forEach((line, i) => {
              ctx.fillText(line, x + 2, y + 10 + i * 9);
            });
          }
        }
      }

      // Draw info panel
      ctx.fillStyle = 'rgba(0, 0, 0, 0.85)';
      ctx.fillRect(10, elements.canvas.height - 130, 220, 120);
      ctx.font = '11px sans-serif';
      ctx.textAlign = 'left';

      // Title
      ctx.fillStyle = '#4ade80';
      ctx.fillText('Smart Wall Detection', 20, elements.canvas.height - 112);

      // Depth status
      ctx.fillStyle = hasDepthData ? '#22d3ee' : '#fbbf24';
      ctx.fillText(hasDepthData ? '‚úì Depth data active' : '‚è≥ Depth loading...', 20, elements.canvas.height - 94);

      // Legend based on mode
      if (vizMode === 'Factor Breakdown') {
        const legendItems = [
          { color: 'rgb(59, 130, 246)', label: 'Uniformity' },
          { color: 'rgb(250, 204, 21)', label: 'Vertical' },
          { color: 'rgb(168, 85, 247)', label: 'Person' },
          { color: 'rgb(34, 211, 238)', label: 'Depth' },
          { color: 'rgb(74, 222, 128)', label: 'Final' }
        ];

        legendItems.forEach((item, i) => {
          ctx.fillStyle = item.color;
          ctx.fillRect(20 + (i % 3) * 70, elements.canvas.height - 75 + Math.floor(i / 3) * 18, 10, 10);
          ctx.fillStyle = '#fff';
          ctx.font = '9px sans-serif';
          ctx.fillText(item.label, 33 + (i % 3) * 70, elements.canvas.height - 66 + Math.floor(i / 3) * 18);
        });
      } else if (vizMode === 'Depth + Color') {
        ctx.fillStyle = '#888';
        ctx.fillText('Green = Color uniformity', 20, elements.canvas.height - 75);
        ctx.fillText('Blue = Depth score', 20, elements.canvas.height - 58);
        ctx.fillText('Cyan = Both high (good wall)', 20, elements.canvas.height - 41);
      } else if (vizMode === 'Depth Flatness') {
        ctx.fillStyle = '#888';
        ctx.fillText('Green = Flat surface (low depth variance)', 20, elements.canvas.height - 75);
        ctx.fillText('Red = Uneven surface', 20, elements.canvas.height - 58);
        ctx.fillText('Numbers = Relative distance', 20, elements.canvas.height - 41);
      } else {
        ctx.fillStyle = '#888';
        ctx.fillText('Green overlay = Detected wall region', 20, elements.canvas.height - 75);
        ctx.fillText('Brighter = Higher confidence', 20, elements.canvas.height - 58);
      }

      // Ceiling boundary
      if (ceilingBoundaryY > 0) {
        ctx.strokeStyle = 'rgba(239, 68, 68, 0.6)';
        ctx.lineWidth = 1;
        ctx.setLineDash([5, 5]);
        ctx.beginPath();
        ctx.moveTo(0, ceilingBoundaryY);
        ctx.lineTo(elements.canvas.width, ceilingBoundaryY);
        ctx.stroke();
        ctx.setLineDash([]);
      }
    }

    function analyzeCellColor(imageData, startX, startY, width, height) {
      let totalR = 0, totalG = 0, totalB = 0;
      let count = 0;

      const sampleStep = 4; // Sample every 4th pixel for performance

      for (let y = startY; y < startY + height && y < imageData.height; y += sampleStep) {
        for (let x = startX; x < startX + width && x < imageData.width; x += sampleStep) {
          const i = (y * imageData.width + x) * 4;
          totalR += imageData.data[i];
          totalG += imageData.data[i + 1];
          totalB += imageData.data[i + 2];
          count++;
        }
      }

      const avgR = totalR / count;
      const avgG = totalG / count;
      const avgB = totalB / count;

      // Calculate variance
      let variance = 0;
      for (let y = startY; y < startY + height && y < imageData.height; y += sampleStep) {
        for (let x = startX; x < startX + width && x < imageData.width; x += sampleStep) {
          const i = (y * imageData.width + x) * 4;
          const diffR = imageData.data[i] - avgR;
          const diffG = imageData.data[i + 1] - avgG;
          const diffB = imageData.data[i + 2] - avgB;
          variance += (diffR * diffR + diffG * diffG + diffB * diffB) / 3;
        }
      }
      variance = Math.sqrt(variance / count);

      return {
        avgColor: { r: avgR, g: avgG, b: avgB },
        variance
      };
    }

    function processEdgeDetection() {
      ctx.drawImage(elements.webcam, 0, 0);

      const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
      const threshold = state.settings['threshold'] || 40;
      const minLength = state.settings['min-length'] || 15;

      // Simple Sobel edge detection
      const edges = detectEdgesSobel(imageData, threshold);

      const vizMode = state.currentModel.vizOptions[state.vizMode];

      ctx.strokeStyle = '#e94560';
      ctx.lineWidth = 2;

      edges.forEach(edge => {
        if (vizMode === 'Strong Edges Only' && edge.strength < threshold * 1.5) return;
        
        ctx.beginPath();
        ctx.moveTo(edge.x1, edge.y1);
        ctx.lineTo(edge.x2, edge.y2);
        
        if (vizMode === 'Edge Angles') {
          // Color by angle
          const hue = (edge.angle + 180) / 360 * 255;
          ctx.strokeStyle = `hsl(${hue}, 80%, 60%)`;
        }
        
        ctx.stroke();
      });
    }

    function detectEdgesSobel(imageData, threshold) {
      const width = imageData.width;
      const height = imageData.height;
      const data = imageData.data;
      const edges = [];

      // Convert to grayscale and apply Sobel
      const gray = new Float32Array(width * height);
      for (let i = 0; i < data.length; i += 4) {
        gray[i / 4] = (data[i] * 0.299 + data[i + 1] * 0.587 + data[i + 2] * 0.114);
      }

      // Sobel kernels
      const sobelX = [-1, 0, 1, -2, 0, 2, -1, 0, 1];
      const sobelY = [-1, -2, -1, 0, 0, 0, 1, 2, 1];

      // Find edge points
      const step = 8; // Sample every 8 pixels
      for (let y = 1; y < height - 1; y += step) {
        for (let x = 1; x < width - 1; x += step) {
          let gx = 0, gy = 0;
          
          for (let ky = -1; ky <= 1; ky++) {
            for (let kx = -1; kx <= 1; kx++) {
              const idx = (y + ky) * width + (x + kx);
              const kidx = (ky + 1) * 3 + (kx + 1);
              gx += gray[idx] * sobelX[kidx];
              gy += gray[idx] * sobelY[kidx];
            }
          }

          const magnitude = Math.sqrt(gx * gx + gy * gy);
          
          if (magnitude > threshold) {
            const angle = Math.atan2(gy, gx) * 180 / Math.PI;
            const length = 15;
            const dx = Math.cos(angle * Math.PI / 180) * length;
            const dy = Math.sin(angle * Math.PI / 180) * length;
            
            edges.push({
              x1: x - dx / 2,
              y1: y - dy / 2,
              x2: x + dx / 2,
              y2: y + dy / 2,
              strength: magnitude,
              angle: angle
            });
          }
        }
      }

      return edges;
    }

    // Cache for depth estimation results
    const depthCache = {
      frameCount: 0,
      depthMap: null,
      depthWidth: 0,
      depthHeight: 0,
      isProcessing: false,
      lastInferenceTime: 0
    };

    function processDepthEstimation() {
      ctx.drawImage(elements.webcam, 0, 0);

      const depthEstimator = state.models.depthEstimator;
      const updateInterval = state.settings['update-interval'] ?? 15;
      const vizMode = state.currentModel.vizOptions[state.vizMode];

      // Check if we should update the depth map
      depthCache.frameCount++;
      const shouldUpdate = (depthCache.frameCount >= updateInterval || !depthCache.depthMap)
                           && !depthCache.isProcessing
                           && depthEstimator;

      if (shouldUpdate) {
        depthCache.frameCount = 0;
        runDepthEstimation();
      }

      // Render using cached depth map
      if (depthCache.depthMap) {
        renderDepthVisualization(vizMode);
      } else {
        // Show status message
        ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
        ctx.fillRect(10, elements.canvas.height - 80, 350, 70);
        ctx.font = '12px sans-serif';
        ctx.textAlign = 'left';

        if (!depthEstimator) {
          ctx.fillStyle = '#fbbf24';
          ctx.fillText('‚è≥ Loading Depth Anything V2 model...', 20, elements.canvas.height - 58);
          ctx.fillStyle = '#888';
          ctx.fillText('First load downloads ~27MB model (cached after)', 20, elements.canvas.height - 40);
          ctx.fillText('Check browser console for progress', 20, elements.canvas.height - 22);
        } else if (depthCache.isProcessing) {
          ctx.fillStyle = '#60a5fa';
          ctx.fillText('üîÑ Running depth estimation...', 20, elements.canvas.height - 58);
          ctx.fillStyle = '#888';
          ctx.fillText('Model loaded, processing first frame', 20, elements.canvas.height - 40);
        } else {
          ctx.fillStyle = '#4ade80';
          ctx.fillText('‚úì Model ready, waiting for first result', 20, elements.canvas.height - 58);
        }
      }
    }

    async function runDepthEstimation() {
      const depthEstimator = state.models.depthEstimator;
      if (!depthEstimator || depthCache.isProcessing) return;

      depthCache.isProcessing = true;
      const startTime = performance.now();

      try {
        // Create a scaled-down canvas for faster inference
        const scale = state.settings['resolution-scale'] ?? 0.5;
        const scaledWidth = Math.round(elements.canvas.width * scale);
        const scaledHeight = Math.round(elements.canvas.height * scale);

        // Create canvas for scaling and convert to data URL
        const tempCanvas = document.createElement('canvas');
        tempCanvas.width = scaledWidth;
        tempCanvas.height = scaledHeight;
        const tempCtx = tempCanvas.getContext('2d');
        tempCtx.drawImage(elements.webcam, 0, 0, scaledWidth, scaledHeight);

        // Convert to data URL (Transformers.js accepts URLs and data URLs)
        const dataUrl = tempCanvas.toDataURL('image/jpeg', 0.8);

        // Run depth estimation
        const result = await depthEstimator(dataUrl);

        console.log('Depth result:', result);

        // Extract depth data from result - output is "predicted_depth" tensor
        if (result && result.predicted_depth) {
          const depthTensor = result.predicted_depth;
          depthCache.depthMap = depthTensor.data;
          depthCache.depthWidth = depthTensor.dims[1]; // dims is [height, width]
          depthCache.depthHeight = depthTensor.dims[0];
          console.log('Depth map updated:', depthCache.depthWidth, 'x', depthCache.depthHeight);
        } else {
          console.warn('Unexpected depth result format:', result);
        }

        depthCache.lastInferenceTime = performance.now() - startTime;
      } catch (error) {
        console.error('Depth estimation error:', error);
      } finally {
        depthCache.isProcessing = false;
      }
    }

    function renderDepthVisualization(vizMode) {
      const { depthMap, depthWidth, depthHeight, lastInferenceTime } = depthCache;
      if (!depthMap) return;

      const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
      const pixels = imageData.data;

      // Find min/max for normalization
      let minDepth = Infinity, maxDepth = -Infinity;
      for (let i = 0; i < depthMap.length; i++) {
        minDepth = Math.min(minDepth, depthMap[i]);
        maxDepth = Math.max(maxDepth, depthMap[i]);
      }
      const depthRange = maxDepth - minDepth || 1;

      // Scale factors for mapping depth map to canvas
      const scaleX = depthWidth / elements.canvas.width;
      const scaleY = depthHeight / elements.canvas.height;

      // Wall plane detection for "Wall Planes" viz mode
      let wallPlanes = null;
      if (vizMode === 'Wall Planes') {
        wallPlanes = detectWallPlanes(depthMap, depthWidth, depthHeight, minDepth, maxDepth);
      }

      for (let y = 0; y < elements.canvas.height; y++) {
        for (let x = 0; x < elements.canvas.width; x++) {
          // Sample depth map (nearest neighbor)
          const dx = Math.floor(x * scaleX);
          const dy = Math.floor(y * scaleY);
          const depthIdx = dy * depthWidth + dx;
          const rawDepth = depthMap[depthIdx] || 0;

          // Normalize depth to 0-1 (closer = lower value in Depth Anything)
          const depth = (rawDepth - minDepth) / depthRange;

          const pixelIndex = (y * elements.canvas.width + x) * 4;

          if (vizMode === 'Depth Map') {
            // Color gradient: red (near) to blue (far)
            const hue = depth * 240;
            const rgb = hslToRgb(hue / 360, 0.8, 0.5);
            pixels[pixelIndex] = rgb[0];
            pixels[pixelIndex + 1] = rgb[1];
            pixels[pixelIndex + 2] = rgb[2];
          } else if (vizMode === 'Depth Contours') {
            // Draw contour lines
            const contourInterval = 0.08;
            const contourValue = depth % contourInterval;
            if (contourValue < 0.008 || contourValue > contourInterval - 0.008) {
              pixels[pixelIndex] = 255;
              pixels[pixelIndex + 1] = 255;
              pixels[pixelIndex + 2] = 255;
            }
          } else if (vizMode === 'Near/Far Regions') {
            // Highlight near (person) and far (wall) regions
            if (depth < 0.35) {
              // Near - red tint
              pixels[pixelIndex] = Math.min(255, pixels[pixelIndex] + 100);
              pixels[pixelIndex + 1] = Math.floor(pixels[pixelIndex + 1] * 0.6);
              pixels[pixelIndex + 2] = Math.floor(pixels[pixelIndex + 2] * 0.6);
            } else if (depth > 0.65) {
              // Far - blue tint
              pixels[pixelIndex] = Math.floor(pixels[pixelIndex] * 0.6);
              pixels[pixelIndex + 1] = Math.floor(pixels[pixelIndex + 1] * 0.6);
              pixels[pixelIndex + 2] = Math.min(255, pixels[pixelIndex + 2] + 100);
            }
          } else if (vizMode === 'Wall Planes' && wallPlanes) {
            // Color by wall plane membership
            const planeId = wallPlanes[dy * depthWidth + dx];
            if (planeId > 0) {
              const planeHue = (planeId * 67) % 360; // Spread colors
              const rgb = hslToRgb(planeHue / 360, 0.7, 0.5);
              pixels[pixelIndex] = Math.floor(pixels[pixelIndex] * 0.4 + rgb[0] * 0.6);
              pixels[pixelIndex + 1] = Math.floor(pixels[pixelIndex + 1] * 0.4 + rgb[1] * 0.6);
              pixels[pixelIndex + 2] = Math.floor(pixels[pixelIndex + 2] * 0.4 + rgb[2] * 0.6);
            }
          }
        }
      }

      ctx.putImageData(imageData, 0, 0);

      // Add info overlay
      ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
      ctx.fillRect(10, elements.canvas.height - 80, 300, 70);
      ctx.fillStyle = '#4ade80';
      ctx.font = '12px sans-serif';
      ctx.textAlign = 'left';
      ctx.fillText('Depth Anything V2 Small', 20, elements.canvas.height - 60);
      ctx.fillStyle = '#888';
      ctx.fillText(`Inference: ${lastInferenceTime.toFixed(0)}ms | Resolution: ${depthWidth}x${depthHeight}`, 20, elements.canvas.height - 42);
      ctx.fillText(`Depth range: ${minDepth.toFixed(2)} - ${maxDepth.toFixed(2)}`, 20, elements.canvas.height - 24);
    }

    // Detect flat wall planes from depth map
    function detectWallPlanes(depthMap, width, height, minDepth, maxDepth) {
      const depthRange = maxDepth - minDepth || 1;
      const planes = new Int32Array(width * height);
      const depthTolerance = depthRange * 0.05; // 5% of depth range

      let planeId = 0;
      const gridSize = 8;
      const cellWidth = Math.floor(width / gridSize);
      const cellHeight = Math.floor(height / gridSize);

      // Analyze grid cells for consistent depth
      for (let gy = 0; gy < gridSize; gy++) {
        for (let gx = 0; gx < gridSize; gx++) {
          const startX = gx * cellWidth;
          const startY = gy * cellHeight;

          // Sample depth values in this cell
          let sumDepth = 0, count = 0;
          for (let y = startY; y < startY + cellHeight && y < height; y += 2) {
            for (let x = startX; x < startX + cellWidth && x < width; x += 2) {
              sumDepth += depthMap[y * width + x];
              count++;
            }
          }
          const avgDepth = sumDepth / count;

          // Calculate variance
          let variance = 0;
          for (let y = startY; y < startY + cellHeight && y < height; y += 2) {
            for (let x = startX; x < startX + cellWidth && x < width; x += 2) {
              const diff = depthMap[y * width + x] - avgDepth;
              variance += diff * diff;
            }
          }
          variance = Math.sqrt(variance / count);

          // If low variance (flat surface), mark as wall plane
          if (variance < depthTolerance) {
            planeId++;
            for (let y = startY; y < startY + cellHeight && y < height; y++) {
              for (let x = startX; x < startX + cellWidth && x < width; x++) {
                planes[y * width + x] = planeId;
              }
            }
          }
        }
      }

      return planes;
    }

    // HSL to RGB helper
    function hslToRgb(h, s, l) {
      let r, g, b;
      if (s === 0) {
        r = g = b = l;
      } else {
        const hue2rgb = (p, q, t) => {
          if (t < 0) t += 1;
          if (t > 1) t -= 1;
          if (t < 1/6) return p + (q - p) * 6 * t;
          if (t < 1/2) return q;
          if (t < 2/3) return p + (q - p) * (2/3 - t) * 6;
          return p;
        };
        const q = l < 0.5 ? l * (1 + s) : l + s - l * s;
        const p = 2 * l - q;
        r = hue2rgb(p, q, h + 1/3);
        g = hue2rgb(p, q, h);
        b = hue2rgb(p, q, h - 1/3);
      }
      return [Math.round(r * 255), Math.round(g * 255), Math.round(b * 255)];
    }

    function processStabilization() {
      ctx.drawImage(elements.webcam, 0, 0);

      // Draw a test pattern to demonstrate stabilization
      const mode = state.settings['mode'] || 'Current (Smoothing)';
      const smoothing = state.settings['smoothing'] || 0.7;

      // Simulated "detected" position with jitter
      const baseX = elements.canvas.width / 2;
      const baseY = elements.canvas.height / 2;
      const jitter = 5;
      const rawX = baseX + (Math.random() - 0.5) * jitter * 2;
      const rawY = baseY + (Math.random() - 0.5) * jitter * 2;

      // Apply stabilization based on mode
      let displayX, displayY;
      
      if (!state.stabilization) {
        state.stabilization = { x: baseX, y: baseY, trail: [] };
      }

      if (mode === 'None') {
        displayX = rawX;
        displayY = rawY;
      } else if (mode === 'Current (Smoothing)') {
        state.stabilization.x = state.stabilization.x * smoothing + rawX * (1 - smoothing);
        state.stabilization.y = state.stabilization.y * smoothing + rawY * (1 - smoothing);
        displayX = state.stabilization.x;
        displayY = state.stabilization.y;
      } else {
        // Feature tracking simulation - more stable
        state.stabilization.x = state.stabilization.x * 0.95 + rawX * 0.05;
        state.stabilization.y = state.stabilization.y * 0.95 + rawY * 0.05;
        displayX = state.stabilization.x;
        displayY = state.stabilization.y;
      }

      // Track trail
      state.stabilization.trail.push({ x: displayX, y: displayY });
      if (state.stabilization.trail.length > 60) {
        state.stabilization.trail.shift();
      }

      const vizMode = state.currentModel.vizOptions[state.vizMode];

      // Draw based on viz mode
      if (vizMode === 'Position Trail') {
        // Draw trail
        ctx.beginPath();
        ctx.strokeStyle = 'rgba(233, 69, 96, 0.5)';
        ctx.lineWidth = 2;
        state.stabilization.trail.forEach((pt, i) => {
          if (i === 0) ctx.moveTo(pt.x, pt.y);
          else ctx.lineTo(pt.x, pt.y);
        });
        ctx.stroke();
      }

      // Draw test art (rectangle)
      const artWidth = 200;
      const artHeight = 150;
      ctx.strokeStyle = '#e94560';
      ctx.lineWidth = 3;
      ctx.strokeRect(displayX - artWidth / 2, displayY - artHeight / 2, artWidth, artHeight);
      
      ctx.fillStyle = 'rgba(233, 69, 96, 0.2)';
      ctx.fillRect(displayX - artWidth / 2, displayY - artHeight / 2, artWidth, artHeight);

      // Label
      ctx.fillStyle = '#fff';
      ctx.font = '14px sans-serif';
      ctx.textAlign = 'center';
      ctx.fillText(`Mode: ${mode}`, displayX, displayY);
    }

    function processCombined() {
      const showSegmentation = state.settings['show-segmentation'] ?? true;
      const showWalls = state.settings['show-walls'] ?? true;
      const showEdges = state.settings['show-edges'] ?? false;

      // Draw base video
      ctx.drawImage(elements.webcam, 0, 0);

      const imageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
      const pixels = imageData.data;

      // 1. Segmentation (if model loaded and enabled)
      let segmentationMask = null;
      if (showSegmentation && state.models.segmenter) {
        const result = state.models.segmenter.segmentForVideo(elements.webcam, performance.now());
        if (result.categoryMask) {
          segmentationMask = result.categoryMask.getAsUint8Array();
          result.categoryMask.close();
        }
      }

      // 2. Wall Detection
      let wallMask = null;
      if (showWalls) {
        wallMask = detectWallsSimple(imageData);
      }

      // Apply combined visualization
      for (let i = 0; i < pixels.length / 4; i++) {
        const pixelIndex = i * 4;
        const isPerson = segmentationMask ? segmentationMask[i] > 0 : false;
        const isWall = wallMask ? wallMask[i] : false;

        if (isPerson) {
          // Person - subtle red tint
          pixels[pixelIndex] = Math.min(255, pixels[pixelIndex] + 30);
        } else if (isWall) {
          // Wall region - green tint
          pixels[pixelIndex + 1] = Math.min(255, pixels[pixelIndex + 1] + 40);
        }
      }

      ctx.putImageData(imageData, 0, 0);

      // 3. Edge Detection overlay
      if (showEdges) {
        const threshold = 40;
        const freshImageData = ctx.getImageData(0, 0, elements.canvas.width, elements.canvas.height);
        const edges = detectEdgesSobel(freshImageData, threshold);

        ctx.strokeStyle = 'rgba(255, 255, 0, 0.6)';
        ctx.lineWidth = 1;
        edges.forEach(edge => {
          ctx.beginPath();
          ctx.moveTo(edge.x1, edge.y1);
          ctx.lineTo(edge.x2, edge.y2);
          ctx.stroke();
        });
      }

      // Draw legend
      ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
      ctx.fillRect(10, 10, 200, 90);
      ctx.font = '12px sans-serif';
      ctx.textAlign = 'left';

      ctx.fillStyle = '#e94560';
      ctx.fillRect(20, 25, 12, 12);
      ctx.fillStyle = '#fff';
      ctx.fillText('Person (Segmentation)', 40, 35);

      ctx.fillStyle = '#4ade80';
      ctx.fillRect(20, 45, 12, 12);
      ctx.fillStyle = '#fff';
      ctx.fillText('Wall Region', 40, 55);

      if (showEdges) {
        ctx.fillStyle = '#fbbf24';
        ctx.fillRect(20, 65, 12, 12);
        ctx.fillStyle = '#fff';
        ctx.fillText('Detected Edges', 40, 75);
      }

      ctx.fillStyle = '#888';
      ctx.font = '10px sans-serif';
      ctx.fillText('Toggle layers in Settings panel', 20, 92);
    }

    // Improved wall detection for combined view
    function detectWallsSimple(imageData) {
      const width = imageData.width;
      const height = imageData.height;
      const data = imageData.data;
      const gridSize = 8;
      const cellWidth = Math.floor(width / gridSize);
      const cellHeight = Math.floor(height / gridSize);
      const wallMask = new Array(width * height).fill(false);

      // Detect ceiling boundary
      const ceilingBoundaryY = detectCeilingBoundarySimple(imageData);

      for (let gy = 0; gy < gridSize; gy++) {
        for (let gx = 0; gx < gridSize; gx++) {
          const startX = gx * cellWidth;
          const startY = gy * cellHeight;
          const centerY = startY + cellHeight / 2;

          // Calculate variance
          let totalR = 0, totalG = 0, totalB = 0, count = 0;
          for (let y = startY; y < startY + cellHeight && y < height; y += 4) {
            for (let x = startX; x < startX + cellWidth && x < width; x += 4) {
              const i = (y * width + x) * 4;
              totalR += data[i];
              totalG += data[i + 1];
              totalB += data[i + 2];
              count++;
            }
          }

          const avgR = totalR / count, avgG = totalG / count, avgB = totalB / count;
          let variance = 0;

          for (let y = startY; y < startY + cellHeight && y < height; y += 4) {
            for (let x = startX; x < startX + cellWidth && x < width; x += 4) {
              const i = (y * width + x) * 4;
              const diffR = data[i] - avgR;
              const diffG = data[i + 1] - avgG;
              const diffB = data[i + 2] - avgB;
              variance += (diffR * diffR + diffG * diffG + diffB * diffB) / 3;
            }
          }
          variance = Math.sqrt(variance / count);

          // Calculate uniformity score
          const uniformityScore = Math.max(0, 1 - variance / 50);

          // Calculate vertical position score (bell curve)
          const normalizedY = gy / gridSize;
          const verticalScore = Math.exp(-Math.pow(normalizedY - 0.5, 2) / (2 * 0.3 * 0.3));

          // Ceiling penalty
          const ceilingPenalty = centerY < ceilingBoundaryY ? 0.3 : 1.0;

          // Combined score
          const wallScore = (uniformityScore * 0.6 + verticalScore * 0.4) * ceilingPenalty;

          // Mark as wall if score is high enough
          if (wallScore > 0.45) {
            for (let y = startY; y < startY + cellHeight && y < height; y++) {
              for (let x = startX; x < startX + cellWidth && x < width; x++) {
                wallMask[y * width + x] = true;
              }
            }
          }
        }
      }

      return wallMask;
    }

    // Simple ceiling boundary detection for combined view
    function detectCeilingBoundarySimple(imageData) {
      const width = imageData.width;
      const height = imageData.height;
      const data = imageData.data;
      const maxY = Math.floor(height * 0.25);

      let strongestEdgeY = 0;
      let strongestEdgeStrength = 0;

      for (let y = 5; y < maxY; y++) {
        let edgeStrength = 0;

        for (let x = 10; x < width - 10; x += 5) {
          const above = ((y - 2) * width + x) * 4;
          const below = ((y + 2) * width + x) * 4;

          const grayAbove = (data[above] + data[above + 1] + data[above + 2]) / 3;
          const grayBelow = (data[below] + data[below + 1] + data[below + 2]) / 3;

          edgeStrength += Math.abs(grayAbove - grayBelow);
        }

        edgeStrength /= Math.floor((width - 20) / 5);

        if (edgeStrength > strongestEdgeStrength && edgeStrength > 15) {
          strongestEdgeStrength = edgeStrength;
          strongestEdgeY = y;
        }
      }

      return strongestEdgeY;
    }

    // ============================================
    // UI Helpers
    // ============================================
    function updateMetrics() {
      elements.fpsDisplay.textContent = `${state.fps} FPS`;
      elements.inferenceDisplay.textContent = `${state.inferenceTime.toFixed(1)} ms`;
      elements.metricFps.textContent = state.fps;
      elements.metricInference.textContent = state.inferenceTime.toFixed(1);
    }

    function showLoading(text = 'Loading...') {
      elements.loadingOverlay.querySelector('.loading-text').textContent = text;
      elements.loadingOverlay.classList.remove('hidden');
    }

    function hideLoading() {
      elements.loadingOverlay.classList.add('hidden');
    }

    function showError(message) {
      elements.errorToast.textContent = message;
      elements.errorToast.classList.add('visible');
      setTimeout(() => {
        elements.errorToast.classList.remove('visible');
      }, 5000);
    }

    function takeScreenshot() {
      const link = document.createElement('a');
      link.download = `vision-playground-${Date.now()}.png`;
      link.href = elements.canvas.toDataURL();
      link.click();
    }

    // ============================================
    // Start
    // ============================================
    init();
  </script>
</body>
</html>
